.gitignore::13
venv/
.idea/__init__.py::0
cli.py::5115
"""
Command-line interface for tlptaco version 2.
"""
import argparse
import sys

# Initialize CLI spinner as early as possible to cover heavy imports
from tlptaco.utils.loading_bar import LoadingSpinner
_spinner = LoadingSpinner()
_spinner.start()
import atexit
# Ensure spinner is stopped on any exit (including --help or errors)
atexit.register(_spinner.stop)

from tlptaco.config.loader import load_config
from tlptaco.db.runner import DBRunner
from tlptaco.engines.eligibility import EligibilityEngine
from tlptaco.engines.waterfall import WaterfallEngine
from tlptaco.engines.output import OutputEngine
from tlptaco.utils.logging import configure_logging


def main():
    parser = argparse.ArgumentParser(description="tlptaco v2: Eligibility → Waterfall → Output pipeline")
    import os
    parser.add_argument("--config", "-c", required=True, help="Path to configuration YAML/JSON file")
    parser.add_argument("--output-dir", "-o", default=None,
                        help="Directory to write outputs and logs (defaults to current working directory)")
    parser.add_argument("--mode", "-m", choices=["full", "presizing"], default="full",
                        help="Run mode: full (includes output) or presizing (eligibility+waterfall only)")
    parser.add_argument("--verbose", "-v", action="store_true", help="Enable verbose (DEBUG) console output")
    parser.add_argument("--progress", "-p", action="store_true", help="Show progress bars for pipeline stages (requires rich)")
    # Stop the initialization spinner before parsing arguments (so help prints cleanly)
    try:
        _spinner.stop()
    except Exception:
        pass
    args = parser.parse_args()

    # Determine working directory for outputs/logs
    workdir = os.path.abspath(args.output_dir) if args.output_dir else os.getcwd()
    os.makedirs(os.path.join(workdir, 'logs'), exist_ok=True)
    # Load configuration
    config = load_config(args.config)
    # ------------------------------------------------------------------
    # Derive default log filenames (include offer_code for easy tracing)
    # ------------------------------------------------------------------
    import re
    safe_offer = re.sub(r'[^A-Za-z0-9_-]+', '_', config.offer_code or 'run')
    logs_dir = os.path.join(workdir, 'logs')

    # Override logging paths to use workdir if not explicitly set
    if not config.logging.file:
        config.logging.file = os.path.join(logs_dir, f'tlptaco_{safe_offer}.log')
    if not config.logging.debug_file:
        config.logging.debug_file = os.path.join(logs_dir, f'tlptaco_{safe_offer}.debug.log')
    # Default SQL log file path
    if not getattr(config.logging, 'sql_file', None):
        config.logging.sql_file = os.path.join(logs_dir, f'tlptaco_{safe_offer}.sql.log')

    # Override waterfall and output paths to live under --output-dir
    # Waterfall output directory
    wf_dir = config.waterfall.output_directory
    if wf_dir and not os.path.isabs(wf_dir):
        config.waterfall.output_directory = os.path.join(workdir, wf_dir)

    # Output channel file locations
    for channel_cfg in config.output.channels.values():
        loc = channel_cfg.file_location
        if loc and not os.path.isabs(loc):
            channel_cfg.file_location = os.path.join(workdir, loc)
    logger = configure_logging(config.logging, verbose=args.verbose)
    runner = DBRunner(config.database, logger)
    # Ensure spinner is stopped after runner is ready
    try:
        _spinner.stop()
    except Exception:
        pass

    # Instantiate engines
    eligibility_engine = EligibilityEngine(config.eligibility, runner, logger)
    waterfall_engine = WaterfallEngine(config.waterfall, runner, logger)
    # Propagate metadata from config
    waterfall_engine.offer_code = config.offer_code
    waterfall_engine.campaign_planner = config.campaign_planner
    waterfall_engine.lead = config.lead
    if args.mode == "full":
        output_engine = OutputEngine(config.output, runner, logger)

    if args.progress:
        # Lazy import of ProgressManager to avoid requiring rich if unused
        from tlptaco.utils.loading_bar import ProgressManager
        # Determine steps for each stage
        elig_steps = eligibility_engine.num_steps()
        wf_steps = waterfall_engine.num_steps(eligibility_engine)
        layers = [("Eligibility", elig_steps), ("Waterfall", wf_steps)]
        if args.mode == "full":
            out_steps = output_engine.num_steps(eligibility_engine)
            layers.append(("Output", out_steps))
        # Run with progress bars
        with ProgressManager(layers, units="steps", title=config.offer_code) as pm:
            eligibility_engine.run(progress=pm)
            waterfall_engine.run(progress=pm)
            if args.mode == "full":
                output_engine.run(progress=pm)
    else:
        # Run without progress bars
        eligibility_engine.run()
        waterfall_engine.run(eligibility_engine)
        if args.mode == "full":
            output_engine.run(eligibility_engine)

    runner.cleanup()

if __name__ == "__main__":
    main()
config/loader.py::878
"""
Load and parse tlptaco configuration files into Pydantic models.
"""
try:
    import yaml
except ImportError:
    yaml = None
import json

from tlptaco.config.schema import AppConfig

# TODO: the yaml needs to be read in order (i.e., OrderedDict since segment order matters)
def load_config(path: str) -> AppConfig:
    """
    Load a YAML or JSON config file and parse into AppConfig.
    """
    if path.lower().endswith(('.yml', '.yaml')):
        if yaml is None:
            raise ImportError("PyYAML is required to load YAML configs; please install pyyaml")
        with open(path, 'r') as f:
            data = yaml.safe_load(f)
    elif path.lower().endswith('.json'):
        with open(path, 'r') as f:
            data = json.load(f)
    else:
        raise ValueError('Unsupported config format, must be .yaml/.yml or .json')

    return AppConfig.parse_obj(data)config/__init__.py::67
"""
Configuration loader and schema definitions for tlptaco v2.
"""config/schema.py::11043
"""
Pydantic models for tlptaco configuration with built-in validation.
"""
from pydantic import BaseModel, model_validator, field_validator, ConfigDict, Field
from typing import List, Dict, Optional, Any, Union
import re

# --- Base Models ---

class ConditionCheck(BaseModel):
    # name is autogenerated if not provided
    name: Optional[str] = None
    sql: str
    description: Optional[str] = None

class TemplateConditions(BaseModel):
    """
    Conditions for a given channel or main: a BA (base) filter and one or more segments.
    Users may define segments directly alongside BA; legacy 'others' key is also supported.
    """
    # allow legacy 'others' key and arbitrary segment keys
    model_config = ConfigDict(extra="allow")
    BA: List[ConditionCheck]
    # segments: mapping segment_name -> list of checks
    segments: Dict[str, List[ConditionCheck]] = {}

    @model_validator(mode='before')
    def parse_segments(cls, data):  # type: ignore[name-defined]
        # Extract user-defined segments: legacy 'others' dict or keys beside 'BA'
        if not isinstance(data, dict):
            return data
        raw = data.copy()
        segs: Dict[str, Any] = {}
        # legacy 'others'
        if 'others' in raw:
            old = raw.pop('others') or {}
            if isinstance(old, dict):
                segs.update(old)
        # new-style: any key != 'BA'
        for key in list(raw.keys()):
            if key == 'BA':
                continue
            # treat as segment
            segs[key] = raw.pop(key)
        raw['segments'] = segs
        return raw
    
    @property
    def others(self) -> Dict[str, List[ConditionCheck]]:
        """Legacy alias for segments mapping."""
        return self.segments

class ConditionsConfig(BaseModel):
    """
    All conditions configuration: main BA filters and per-channel BA filters + segments.
    Automatically assigns unique names to each check and enforces structure.
    """
    main: TemplateConditions
    channels: Dict[str, TemplateConditions]

    @model_validator(mode='after')
    def assign_names_and_validate(cls, self) -> 'ConditionsConfig':  # type: ignore[name-defined]
        # Assign autogenerated names for main BA checks
        # main channel key is 'main'
        # BA filters for main
        for idx, chk in enumerate(self.main.BA, start=1):
            chk.name = f"main_BA_{idx}"
        # main should not have any segments
        if self.main.segments:
            raise ValueError("'main' conditions may not include segments; only BA checks are allowed")
        # Process each channel: assign names for BA filters and optional segments
        for channel, tmpl in self.channels.items():
            # BA filters for channel
            for idx, chk in enumerate(tmpl.BA, start=1):
                chk.name = f"{channel}_BA_{idx}"
            # segments for channel (optional)
            for seg_name, checks in tmpl.segments.items():
                for idx, chk in enumerate(checks, start=1):
                    chk.name = f"{channel}_{seg_name}_{idx}"
        return self

class TableConfig(BaseModel):
    name: str
    alias: str
    sql: Optional[str] = None  # Made optional as it might not always be used
    join_type: Optional[str]
    join_conditions: Optional[str]
    where_conditions: Optional[str]
    unique_index: Optional[str]
    collect_stats: Optional[List[str]]
    # Ensure alias is a valid identifier (starts with letter/_ and contains only alphanumeric/_)
    @field_validator('alias', mode='before')
    def validate_alias(cls, v):  # type: ignore[name-defined]
        if not isinstance(v, str) or not re.match(r'^[A-Za-z_][A-Za-z0-9_]*$', v):
            raise ValueError(
                f"Invalid alias '{v}'. Must start with a letter or underscore and contain only alphanumeric characters or underscores."
            )
        return v

class OutputOptions(BaseModel):
    format: str
    additional_arguments: Optional[Dict[str, Any]] = {}
    custom_function: Optional[str]

# --- Config Sections with Validation ---

class EligibilityConfig(BaseModel):
    eligibility_table: str
    conditions: ConditionsConfig
    tables: List[TableConfig]
    unique_identifiers: List[str]

    @model_validator(mode='after')
    def check_identifier_aliases_are_valid(self) -> 'EligibilityConfig':
        valid_aliases = {t.alias for t in self.tables}
        for identifier in self.unique_identifiers:
            if '.' in identifier:
                alias = identifier.split('.')[0]
                if alias not in valid_aliases:
                    raise ValueError(
                        f"In 'eligibility.unique_identifiers', '{identifier}' uses an invalid alias '{alias}'. "
                        f"Valid aliases are: {valid_aliases}"
                    )
        return self
    # Validate eligibility_table naming: identifier or schema.table
    @field_validator('eligibility_table', mode='before')
    def validate_eligibility_table(cls, v):  # type: ignore[name-defined]
        if not isinstance(v, str) or not re.match(
            r'^[A-Za-z_][A-Za-z0-9_]*(\.[A-Za-z_][A-Za-z0-9_]*)?$', v
        ):
            raise ValueError(
                f"Invalid eligibility_table '{v}'. Must be a valid identifier or schema.table."
            )
        return v

# -----------------------------------------------------------------------------
# Waterfall configuration & history tracking
# -----------------------------------------------------------------------------


class WaterfallHistoryConfig(BaseModel):
    """Optional SQLite history tracking for each waterfall run.

    Two *alternative* ways to choose the comparison run:

    1. ``recent_window_days`` (alias ``lookback_days`` – legacy):
       pick the **latest** run whose timestamp is within the last *N* days.
    2. ``compare_offset_days`` (alias ``days_ago_to_compare``):
       pick the run whose timestamp is **closest to exactly N days ago**.
       When this value is supplied it takes precedence over
       ``recent_window_days``.
    """

    model_config = ConfigDict(populate_by_name=True)

    # Toggle history tracking on / off (default off for backwards-compat)
    track: bool = False
    # Optional explicit path to the SQLite DB (directory will be created).
    # When *None*, a file named ``waterfall_history.sqlite`` is created
    # inside ``waterfall.output_directory``.
    db_path: Optional[str] = None

    # Window-style selection (legacy name: lookback_days)
    recent_window_days: Optional[int] = Field(30, alias="lookback_days")

    # Point-in-time offset selection (legacy name: days_ago_to_compare)
    compare_offset_days: Optional[int] = Field(None, alias="days_ago_to_compare")

    @model_validator(mode="after")
    def _validate_numbers(cls, self):  # type: ignore[name-defined]
        if self.track and self.recent_window_days is None and self.compare_offset_days is None:
            # Default window when nothing specified
            self.recent_window_days = 30

        if self.recent_window_days is not None and self.recent_window_days <= 0:
            raise ValueError("recent_window_days must be a positive integer")

        if self.compare_offset_days is not None and self.compare_offset_days <= 0:
            raise ValueError("compare_offset_days must be a positive integer if supplied")
        return self


class WaterfallConfig(BaseModel):
    output_directory: str
    count_columns: List[Union[str, List[str]]]
    # New nested history configuration with sane defaults
    history: WaterfallHistoryConfig = WaterfallHistoryConfig()

class OutputChannelConfig(BaseModel):
    columns: List[str]
    file_location: str
    file_base_name: str
    output_options: OutputOptions
    unique_on: Optional[List[str]] = []

    @model_validator(mode='after')
    def check_unique_on_are_in_columns(cls, self) -> 'OutputChannelConfig':  # type: ignore[name-defined]
        """
        Ensure that simple unique_on column names exist in the output columns.
        If the unique_on element contains a dot, assume it's a qualified identifier and skip validation.
        """
        if self.unique_on:
            # If all unique_on entries are unqualified (no dot), enforce subset
            simple_keys = [u for u in self.unique_on if '.' not in u]
            if simple_keys:
                missing = set(simple_keys) - set(self.columns)
                if missing:
                    raise ValueError(
                        f"'unique_on' columns {missing} are not present in the selected 'columns'."
                    )
        return self

class OutputConfig(BaseModel):
    channels: Dict[str, OutputChannelConfig]

class LoggingConfig(BaseModel):
    level: str
    file: Optional[str]
    debug_file: Optional[str]
    # New dedicated SQL log file capturing rendered SQL from templates
    sql_file: Optional[str] = None

class DatabaseConfig(BaseModel):
    host: str
    user: str
    password: Optional[str]
    logmech: Optional[str] = "KRB5"

# --- Top-Level App Config with Cross-Section Validation ---

class AppConfig(BaseModel):
    """
    Top-level application configuration, including offer code and campaign metadata.
    """
    # Offer code displayed in CLI progress bar (defaults to 'Running')
    offer_code: str = "Running"
    # Campaign metadata for waterfall report header
    campaign_planner: str = ""
    lead: str = ""
    logging: LoggingConfig
    database: DatabaseConfig
    eligibility: EligibilityConfig
    waterfall: WaterfallConfig
    output: OutputConfig

    @model_validator(mode='after')
    def check_cross_config_dependencies(self) -> 'AppConfig':
        # 1. Check WaterfallConfig -> EligibilityConfig dependency
        valid_ids = {uid.split('.')[-1] for uid in self.eligibility.unique_identifiers}
        waterfall_ids = set()
        for item in self.waterfall.count_columns:
            cols = [item] if isinstance(item, str) else item
            for col in cols:
                waterfall_ids.add(col.split('.')[-1])
        if not waterfall_ids.issubset(valid_ids):
            invalid_cols = waterfall_ids - valid_ids
            raise ValueError(
                f"Waterfall 'count_columns' contain invalid identifiers: {invalid_cols}. "
                f"They must be a subset of eligibility 'unique_identifiers': {valid_ids}"
            )

        # 2. Check OutputConfig -> EligibilityConfig dependency (for aliases)
        valid_aliases = {t.alias for t in self.eligibility.tables}
        for channel, out_cfg in self.output.channels.items():
            for column in out_cfg.columns:
                if '.' in column:
                    alias = column.split('.')[0]
                    if alias not in valid_aliases:
                        raise ValueError(
                            f"In output channel '{channel}', column '{column}' uses an invalid alias '{alias}'. "
                            f"Valid aliases are: {valid_aliases}"
                        )
        return self
utils/loading_bar.py::17086
"""
loading_bar.py
--------------------------------

A Rich-powered utility that mimics Docker-style multi-layer progress with a
single **aggregate** bar on top and any number of **layer** bars underneath.

New in this version
~~~~~~~~~~~~~~~~~~~
* **Unit agnostic** – track *bytes* **or** *arbitrary steps* (e.g. epochs,
  items processed, test cases run). Switch with the `units` arg.
* **IDE Compatibility** - Works correctly in IDE terminals (like PyCharm)
  by forcing a live display.
* **Graceful Exit** - Handles `Ctrl+C` (KeyboardInterrupt) cleanly, showing a
  custom spinner with shutdown messages before exiting.
* **Robust Threaded Design** - The main logic now runs in a worker thread,
  allowing the main thread to remain responsive and catch `Ctrl+C` instantly,
  even during blocking I/O operations (like database calls).
* Clean, single-Live implementation (avoids *rich.errors.LiveError*).
* Minimal public API: call `simulate()` or import the helpers into your own
  workflow.

Quick start
-----------
```
pip install rich
python loading_bar.py            # bytes demo (default)
python loading_bar.py steps      # steps demo
```
ASCII-only fallback (environment variable):
```
export LOADING_BAR_ASCII=1
python loading_bar.py steps      # forces ASCII-only display
```
CLI flag fallback:
```
python loading_bar.py steps --ascii  # forces ASCII-only display via flag
```

You can also `import simulate` and feed it your own task list.

"""

from __future__ import annotations
# Ensure script resolves imports from project root when executed directly,
# preventing local modules from shadowing stdlib modules.
import sys, os
_script_dir = os.path.dirname(__file__)
_project_root = os.path.abspath(os.path.join(_script_dir, os.pardir, os.pardir, os.pardir))
if sys.path and sys.path[0] == _script_dir:
    sys.path[0] = _project_root

import random
import time
import threading
import sys
import os
from typing import List, Tuple

from rich.console import Console, Group
from rich.live import Live
from rich.progress import (
    BarColumn,
    DownloadColumn,
    Progress,
    SpinnerColumn,
    TextColumn,
    TimeRemainingColumn,
    TransferSpeedColumn,
)

# ────────────────────────────────────────────────────────────────────────────────
# Helpers
# ────────────────────────────────────────────────────────────────────────────────


def _build_columns(units: str, overall: bool = False, title: str = "Running"):
    """Return a tuple of Rich column objects for *bytes* or *steps* mode.
    When overall=True, the first column shows the given title."""

    if units == "bytes":
        if overall:
            return (
                TextColumn(f"[bold green][+] {title}", justify="right"),
                BarColumn(bar_width=None, complete_style="cyan"),
                DownloadColumn(binary_units=True),
                TimeRemainingColumn(),
            )
        return (
            SpinnerColumn(style="bold magenta"),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(bar_width=None),
            DownloadColumn(binary_units=True),
            TransferSpeedColumn(),
            TimeRemainingColumn(),
        )

    # steps mode
    if overall:
        return (
            TextColumn(f"[bold green][+] {title}", justify="right"),
            BarColumn(bar_width=None, complete_style="cyan"),
            TextColumn("[progress.percentage]{task.completed}/{task.total} steps"),
            TimeRemainingColumn(),
        )
    return (
        SpinnerColumn(style="bold magenta"),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(bar_width=None),
        TextColumn("{task.completed}/{task.total} steps"),
        TimeRemainingColumn(),
    )

# ────────────────────────────────────────────────────────────────────────────────
# ProgressManager: reusable class for multi-layer progress bars
# ────────────────────────────────────────────────────────────────────────────────
class ProgressManager:
    """Manage a multi-layer progress display with an overall bar and individual layer bars.

    Parameters
    ----------
    layers : List[Tuple[str, int]]
        List of (label, total) pairs for each layer.
    units : str
        "bytes" or "steps", controls display style.
    """

    def __init__(self,
                 layers: List[Tuple[str, int]],
                 *,
                 units: str = "steps",
                 title: str = "Running",
                 ascii: bool = False):
        """Manage a multi-layer progress display with an overall bar and individual layer bars.

        Parameters
        ----------
        layers : list[tuple[str, int]]
            (label, total) pairs for each layer.
        units : {"bytes", "steps"}
            Controls display style.
        title : str
            Title text shown next to the overall progress bar.
        """
        if units not in {"bytes", "steps"}:
            raise ValueError("units must be 'bytes' or 'steps'")
        # Determine if terminal supports interactive progress
        self.console = Console()
        term_env = os.environ.get('TERM', '')
        rich_supported = self.console.is_terminal and term_env.lower() != 'dumb'
        ascii_env = os.environ.get('LOADING_BAR_ASCII', '').lower() in ('1', 'true', 'yes', 'y')
        # Determine mode: rich if supported and not forced ascii
        self.use_rich = rich_supported and not ascii and not ascii_env
        self.ascii_mode = not self.use_rich
        self.units = units
        self.title = title
        self.grand_total = sum(total for _, total in layers)
        if self.use_rich:
            # Rich interactive mode
            self.overall = Progress(*_build_columns(units, overall=True, title=title), console=self.console)
            self.layers = Progress(*_build_columns(units), console=self.console)
            self.total_task = self.overall.add_task("overall", total=self.grand_total)
            self.task_ids = {name: self.layers.add_task(name, total=total) for name, total in layers}
            self.finished = False
            self.layout = Group(self.overall, self.layers)
            self.live = None
        else:
            # ASCII fallback mode
            # Track simple progress counts
            self.layer_totals = {name: total for name, total in layers}
            self.layer_completed = {name: 0 for name, _ in layers}
            self.overall_completed = 0
            self.task_ids = None
            self.live = None
            self.finished = self.overall_completed >= self.grand_total

    def __enter__(self):
        if self.use_rich:
            self.live = Live(self.layout, console=self.console, refresh_per_second=10)
            self.live.__enter__()
        return self

    def update(self, layer_name: str, advance: int = 1):
        """Advance the given layer and the overall bar by the specified amount."""
        if self.use_rich:
            task_id = self.task_ids.get(layer_name)
            if task_id is None:
                raise KeyError(f"Unknown layer '{layer_name}'")
            self.layers.update(task_id, advance=advance)
            self.overall.update(self.total_task, advance=advance)
            self.finished = all(task.finished for task in self.layers.tasks)
        elif self.ascii_mode:
            if layer_name not in self.layer_completed:
                raise KeyError(f"Unknown layer '{layer_name}'")
            self.layer_completed[layer_name] += advance
            self.overall_completed += advance
            parts = [f"{self.title}: {self.overall_completed}/{self.grand_total}"]
            for name, total in self.layer_totals.items():
                comp = self.layer_completed.get(name, 0)
                parts.append(f"{name}: {comp}/{total}")
            line = " | ".join(parts)
            try:
                sys.stdout.write("\r" + line)
                sys.stdout.flush()
            except Exception:
                pass
            self.finished = self.overall_completed >= self.grand_total
        else:
            return


    def __exit__(self, exc_type, exc, tb):
        if self.use_rich:
            if self.live:
                self.live.__exit__(exc_type, exc, tb)
        elif self.ascii_mode:
            try:
                sys.stdout.write("\n")
                sys.stdout.flush()
            except Exception:
                pass

# ────────────────────────────────────────────────────────────────────────────────
# Worker Function
# ────────────────────────────────────────────────────────────────────────────────

def worker_function(
    progress_manager: ProgressManager,
    layers: List[Tuple[str, int]],
    units: str,
    stop_event: threading.Event
):
    """This function contains the actual work being done.

    In a real application, this is where you would put your long-running,
    blocking calls (e.g., database queries, file processing).
    """
    # Main work loop: supports both rich and ASCII modes
    while not progress_manager.finished:
        if stop_event.is_set():
            return
        for name, size in layers:
            # For rich mode, skip finished layers
            if progress_manager.use_rich:
                task_id = progress_manager.task_ids.get(name)
                if task_id is None or progress_manager.layers.tasks[task_id].finished:
                    continue
            # Determine work chunk
            if units == "bytes":
                chunk = random.randint(200_000, 2_000_000)
            else:
                chunk = random.randint(1, max(1, size // 100))
            # Update progress (rich or ASCII handles internally)
            progress_manager.update(name, advance=chunk)
        # Simulate latency
        time.sleep(0.05)


# ────────────────────────────────────────────────────────────────────────────────
# CLI initialization spinner with funny snippets
# ────────────────────────────────────────────────────────────────────────────────
DEFAULT_SNIPPETS = [
    "Reticulating splines...",
    "Polishing the flux capacitor...",
    "Negotiating with the server elves...",
    "Counting to infinity...",
    "Charging the warp drive...",
    "Tickling the hamsters...",
    "Aligning bits to bytes...",
    "Spinning up the fun...",
    "Herding cats...",
    "Reheating pizza..."
]

class LoadingSpinner:
    """Simple CLI spinner with rotating missing dot and funny snippets."""

    def __init__(self, interval: float = 0.2, snippet_interval: float = 2.0, snippets: list[str] | None = None):
        # Enable spinner only on interactive terminals
        self.enabled = sys.stdout.isatty() and os.environ.get('TERM', '').lower() != 'dumb'
        self.interval = interval
        self.snippet_interval = snippet_interval
        self.snippets = snippets or DEFAULT_SNIPPETS
        self._stop_event = threading.Event()
        self._thread = None

    def _generate_frames(self) -> list[str]:
        frames = []
        total = 6
        for miss in range(total):
            symbols = ["●" if i != miss else " " for i in range(total)]
            frames.append("[" + "".join(symbols) + "]")
        return frames

    def _spin(self):
        frames = self._generate_frames()
        frame_count = len(frames)
        snippet_count = len(self.snippets)
        idx_frame = idx_snip = 0
        last_snip_time = time.time()
        sys.stdout.write("\r" + " " * 80 + "\r")
        sys.stdout.flush()
        while not self._stop_event.is_set():
            now = time.time()
            if now - last_snip_time >= self.snippet_interval:
                idx_snip = random.randrange(snippet_count)
                last_snip_time = now
            frame = frames[idx_frame]
            snippet = self.snippets[idx_snip]
            text = f"{frame} {snippet}"
            sys.stdout.write("\r" + text.ljust(80))
            sys.stdout.flush()
            time.sleep(self.interval)
            idx_frame = (idx_frame + 1) % frame_count

    def start(self):
        """Start the spinner in a background thread."""
        # Only start spinner if terminal supports it
        if not getattr(self, 'enabled', False):
            return
        if self._thread and self._thread.is_alive():
            return
        self._stop_event.clear()
        self._thread = threading.Thread(target=self._spin, daemon=True)
        self._thread.start()

    def stop(self):
        """Stop spinner and move to next line."""
        # Only stop spinner if it was started
        if not getattr(self, 'enabled', False):
            return
        self._stop_event.set()
        if self._thread:
            self._thread.join()
        # Clear spinner line
        try:
            sys.stdout.write("\r" + " " * 80 + "\r")
            sys.stdout.flush()
        except Exception:
            pass

    def __enter__(self):
        self.start()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.stop()


# ────────────────────────────────────────────────────────────────────────────────
# CLI demo
# ────────────────────────────────────────────────────────────────────────────────


if __name__ == "__main__":
    import sys

    # Determine mode and optional ASCII flag
    args = sys.argv[1:]
    mode = args[0].lower() if args and not args[0].startswith("--") else "bytes"
    ascii_flag = any(arg in ("--ascii",) for arg in args)

    DEMO_LAYERS = [
        ("a9c6b9f9: Pulling fs layer", 120_000_000 if mode == "bytes" else 500),
        ("5b4d5e38: Pulling fs layer", 180_000_000 if mode == "bytes" else 800),
        ("d18f1171: Downloading", 100_000_000 if mode == "bytes" else 400),
        ("e2f3c1d2: Downloading", 160_000_000 if mode == "bytes" else 700),
    ]

    # Set up the progress manager and threading events
    pm = ProgressManager(DEMO_LAYERS, units=mode, ascii=ascii_flag)
    stop_event = threading.Event()

    # Set up and start the worker thread. It's a daemon so it will exit
    # when the main thread exits.
    worker = threading.Thread(
        target=worker_function,
        args=(pm, DEMO_LAYERS, mode, stop_event),
        daemon=True
    )

    try:
        # Use the ProgressManager as a context manager to handle the Live display
        with pm:
            worker.start()
            # This loop keeps the main thread alive and responsive to Ctrl+C
            # while the worker thread does its job.
            while worker.is_alive():
                # We use a non-blocking join to prevent this loop from
                # locking up the main thread.
                worker.join(timeout=0.1)

    except KeyboardInterrupt:
        # This block now runs immediately when you press Ctrl+C.
        console = Console()
        console.print("\n[bold yellow]Interruption received. Telling worker to stop...[/bold yellow]")
        stop_event.set()
        # Give the worker a moment to shut down
        worker.join()

        # Now run the shutdown spinner for a nice exit effect
        shutdown_snippets = [
            "Cleaning up...",
            "Putting the tools away...",
            "Turning off the lights...",
            "One moment...",
            "Shutting down gracefully...",
        ]
        with LoadingSpinner(snippets=shutdown_snippets):
            time.sleep(2)
        print("Process interrupted. Exiting.")
        sys.exit(0)

    except Exception as e:
        console = Console()
        console.print_exception()
        sys.exit(1)

utils/__init__.py::41
"""
Utility functions for tlptaco v2.
"""utils/logging.py::5318
"""
Configure project-wide logging.
"""
import logging
try:
    from rich.logging import RichHandler
    from rich.text import Text
except ImportError:
    RichHandler = None
    Text = None

# Emoji icons for log levels to enhance readability
LEVEL_EMOJI = {
    "DEBUG":    "🐛",
    "INFO":     "ℹ️",
    "WARNING":  "⚠️",
    "ERROR":    "❌",
    "CRITICAL": "🔥",
}

class EmojiFormatter(logging.Formatter):
    """
    Logging Formatter that injects an emoji based on the log level.
    """
    def format(self, record):
        # Attach emoji for the level
        record.emoji = LEVEL_EMOJI.get(record.levelname, "")
        return super().format(record)

def configure_logging(cfg, verbose=False):
    """
    Configure root logger:
      - console handler at DEBUG if verbose, else cfg.level
      - file handler at cfg.level if cfg.file
      - debug file handler at DEBUG if cfg.debug_file
    Returns the root logger.
    """
    root = logging.getLogger()
    root.setLevel(logging.DEBUG)
    from datetime import datetime

    # Prepare EmojiFormatter for file handlers or fallback console
    fmt_str = "%(emoji)s %(asctime)s %(name)s %(levelname)s: %(message)s"
    fmt = EmojiFormatter(fmt_str, datefmt="[%X]")
    # Optionally add console handler only if verbose flag is set
    if verbose:
        # Determine console log level
        console_level = logging.DEBUG
        # Console handler: use EmojiRichHandler (with emojis) if Rich is available, else fallback
        if RichHandler is not None and Text is not None:
            # Subclass RichHandler to prefix level names with emoji
            class EmojiRichHandler(RichHandler):  # type: ignore
                def get_level_text(self, record):  # noqa: A003
                    level = record.levelname
                    style = f"logging.level.{level.lower()}"
                    emoji = LEVEL_EMOJI.get(level, "")
                    # pad level name to width 8
                    padded = level.ljust(8)
                    return Text.assemble((emoji + ' ' + padded, style))
            rich_handler = EmojiRichHandler(
                level=console_level,
                markup=True,
                show_time=True,
                show_level=True,
                show_path=False,
            )
            root.addHandler(rich_handler)
        else:
            ch = logging.StreamHandler()
            ch.setLevel(console_level)
            ch.setFormatter(fmt)
            root.addHandler(ch)
    # File handler
    if getattr(cfg, 'file', None):
        fh = logging.FileHandler(cfg.file)
        file_level = getattr(logging, cfg.level.upper(), logging.INFO)
        fh.setLevel(file_level)
        fh.setFormatter(fmt)
        root.addHandler(fh)
    # Debug file handler
    if getattr(cfg, 'debug_file', None):
        dfh = logging.FileHandler(cfg.debug_file)
        dfh.setLevel(logging.DEBUG)
        dfh.setFormatter(fmt)
        root.addHandler(dfh)

    # ---------------------------------------------------------------------
    # Dedicated SQL logger – captures raw SQL strings for easy copy-paste
    # ---------------------------------------------------------------------
    if getattr(cfg, 'sql_file', None):
        # Ensure directory exists
        import os
        os.makedirs(os.path.dirname(cfg.sql_file), exist_ok=True)

        sql_logger = logging.getLogger('tlptaco.sql')
        sql_logger.setLevel(logging.INFO)
        # Prevent propagation so SQL lines don't double-write to root
        sql_logger.propagate = False
        sql_fh = logging.FileHandler(cfg.sql_file)
        # Use plain formatter: message only – keeps SQL clean for copy-paste
        sql_fh.setFormatter(logging.Formatter('%(message)s'))
        sql_logger.addHandler(sql_fh)

    # ------------------------------------------------------------------
    # Insert a big ASCII header at the beginning of *every* log file to
    # delineate individual tlptaco runs.
    # ------------------------------------------------------------------
    run_header = (
        "=" * 80 +
        f"\nTLPTACO RUN START {datetime.now():%Y-%m-%d %H:%M:%S}\n" +
        "=" * 80
    )
    root.info(run_header)
    # Also write to SQL logger if present
    sql_logger = logging.getLogger('tlptaco.sql')
    if sql_logger.handlers:
        sql_logger.info(run_header)

    return root


# -------------------------------------------------------------------------
# Helper to log rendered SQL under a clear ASCII header
# -------------------------------------------------------------------------

def log_sql_section(section: str, sql_text: str):
    """Write rendered SQL to the dedicated SQL log (if configured).

    Parameters
    ----------
    section : str
        Logical section name (e.g. "Eligibility", "Waterfall", "Output").
    sql_text : str
        Raw SQL text to be logged.
    """
    logger = logging.getLogger('tlptaco.sql')
    if not logger.handlers:
        # SQL logger not configured – nothing to do
        return
    header_line = '#' * 80
    logger.info(header_line)
    logger.info(f"# {section.upper()} SQL")
    logger.info(header_line)
    logger.info(sql_text.strip())
    logger.info('')  # blank line separator

def get_logger(name: str):
    return logging.getLogger(f"tlptaco.{name}")utils/validation.py::184
"""
Custom validation utilities (if any) complementing Pydantic.
"""
# In most cases Pydantic models in config/schema.py will validate structure.
# Add helper functions here as needed.db/connection.py::2181
"""
Wrap Teradata (and other) connections for SQL execution and data transfer.
"""
import teradatasql
import pandas as pd
import warnings
from typing import Any

class DBConnection:
    # TODO add some logging outputs OR extend Daniel's connection
    def __init__(self, host: str, user: str, password: str, logmech: str = "KRB5"):
        self.host = host
        self.user = user
        self.password = password
        self.logmech = logmech
        self.conn = None

    def connect(self):
        # Establish a direct teradatasql (DB-API) connection
        # Build connection arguments
        conn_kwargs = {
            'host': self.host,
            'user': self.user,
            'password': self.password,
        }
        # Include logmech if explicitly set (None = omit)
        if self.logmech is not None:
            conn_kwargs['logmech'] = self.logmech
        # Establish a direct teradatasql (DB-API) connection
        self.conn = teradatasql.connect(**conn_kwargs)

    def disconnect(self):
        if self.conn:
            try:
                self.conn.close()
            except Exception:
                pass
            self.conn = None

    def execute(self, sql: str) -> Any:
        if self.conn is None:
            self.connect()
        cur = self.conn.cursor()
        cur.execute(sql)
        # Commit DDL/DML to the database
        try:
            self.conn.commit()
        except Exception:
            # Some drivers auto-commit or may not support explicit commit
            pass
        return cur

    def to_df(self, sql: str):
        if self.conn is None:
            self.connect()
        # Use pandas to read SQL via DB-API connection
        # Suppress pandas warning about non-SQLAlchemy DBAPI2 connections
        with warnings.catch_warnings():
            warnings.filterwarnings(
                "ignore",
                category=UserWarning,
                message=r"pandas only supports SQLAlchemy connectable.*"
            )
            df = pd.read_sql(sql, self.conn)
        return df

    def fastload(self, df, **kwargs):
        raise NotImplementedError("fastload is not supported with teradatasql driver")
db/__init__.py::54
"""
Database connection and runner for tlptaco v2.
"""db/runner.py::1778
"""
Simple runner to orchestrate multiple SQL executions.
"""
import time
from typing import List
from tlptaco.db.connection import DBConnection

class DBRunner:
    # TODO add timings to logger
    def __init__(self, cfg, logger):
        db = cfg
        self.conn = DBConnection(db.host, db.user, db.password, db.logmech)
        self.logger = logger

    def run(self, sql: str):
        """
        Execute a SQL statement, log the SQL text and timing, and return the cursor.
        """
        start = time.time()
        self.logger.info("Executing SQL statement")
        self.logger.debug(sql)
        cur = self.conn.execute(sql)
        duration = time.time() - start
        self.logger.info(f"SQL execution finished in {duration:.2f}s")
        return cur

    def run_many(self, sql_list: List[str]):
        results = []
        for s in sql_list:
            results.append(self.run(s))
        return results

    def to_df(self, sql: str):
        """
        Execute a SQL query and return a pandas DataFrame, logging SQL text, timing, and shape.
        """
        start = time.time()
        self.logger.info("Fetching data to DataFrame")
        self.logger.debug(sql)
        df = self.conn.to_df(sql)
        duration = time.time() - start
        try:
            rows, cols = df.shape
            self.logger.info(f"Fetched DataFrame with {rows} rows and {cols} columns in {duration:.2f}s")
        except Exception:
            self.logger.info(f"Fetched DataFrame in {duration:.2f}s")
        return df

    def fastload(self, df, **kwargs):
        self.logger.info("Fastloading DataFrame")
        return self.conn.fastload(df, **kwargs)

    def cleanup(self):
        self.logger.info("Cleaning up DB connection")
        self.conn.disconnect()sql/generator.py::1507
"""
Render SQL from Jinja2 templates with provided context.
"""
import os
try:
    from jinja2 import Environment, FileSystemLoader, select_autoescape
except ImportError:
    Environment = FileSystemLoader = select_autoescape = None

class SQLGenerator:
    def __init__(self, templates_dir: str):
        if Environment is None:
            raise ImportError("jinja2 is required to render SQL templates; please install jinja2")
        # Prepare Jinja environment
        self.env = Environment(
            loader=FileSystemLoader(templates_dir),
            autoescape=select_autoescape(["sql", "jinja"])
        )
        # No version or commit tracking in SQL generation (removed per user request)

    def render(self, template_name: str, context: dict) -> str:
        """
        Render the named SQL template with the provided context and return raw SQL.
        """
        tmpl = self.env.get_template(template_name)
        return tmpl.render(**context)

    def list_templates(self, filter_func=None) -> list[str]:  # noqa: F821
        """
        List available SQL templates in the environment.
        By default, only files ending with '.sql.j2' are returned.
        Optionally, apply a filter_func(name) to further filter template names.
        """
        templates = [name for name in self.env.list_templates()
                     if name.endswith('.sql.j2')]
        if filter_func:
            templates = [name for name in templates if filter_func(name)]
        return templates
sql/__init__.py::48
"""
SQL generation utilities for tlptaco v2.
"""sql/templates/eligibility.sql.j2::1208
-- Jinja2 template for eligibility SQL

-- Context:
-- eligibility_table: name of final eligibility table
-- unique_identifiers: list of identifiers with alias (e.g. ['a.col', 'b.col'])
-- unique_without_aliases: list of column names only (e.g. ['col'])
-- checks: list of dicts {name: column_name, sql: expression}
-- tables: list of dicts {name, alias, join_type, join_conditions}
-- where_clauses: list of strings

CREATE TABLE {{ eligibility_table }} AS (
SELECT
{%- for uid in unique_identifiers %}
{{ uid }}{{ "," if not loop.last }}
{%- endfor %}
{%- if checks %},
{%- for check in checks %}
CASE WHEN {{ check.sql }} THEN 1 ELSE 0 END AS {{ check.name }}{{ "," if not loop.last }}
{%- endfor %}
{%- endif %}
FROM {{ tables[0].name }} {{ tables[0].alias }}
{%- for t in tables[1:] %}
{{ t.join_type }} {{ t.name }} {{ t.alias }}
ON {{ t.join_conditions }}
{%- endfor %}
{%- if where_clauses %}
WHERE
{%- for wc in where_clauses %}
{{ wc }}{{ " AND" if not loop.last }}
{%- endfor %}
{%- endif %}
) WITH DATA PRIMARY INDEX prindx ({{ unique_without_aliases|join(', ') }});

-- MODIFIED: Collect stats only on the primary index for robustness.
COLLECT STATISTICS INDEX prindx ON {{ eligibility_table }};sql/templates/output.sql.j2::1435
-- Jinja2 template for final channel output with "Claim and Exclude" logic and deduplication.
--
-- Context:
--   columns: list of columns to select (e.g. ['c.id', 'c.name', ...])
--   eligibility_table: name of the smart eligibility table.
--   cases: A list of dicts, ordered by priority, for the CASE statement.
--          Each dict: {condition: SQL condition string, template: template_name string}
--   unique_on: A list of columns to partition by for deduplication (e.g., ['c.customer_id']).
--
SELECT
    {{ columns|join(',\n    ') }},
    template_id
FROM (
    SELECT
        c.*,
        -- This CASE statement is the core of the "Claim and Exclude" logic.
        -- Because it stops at the first WHEN that is true, the order of the 'cases'
        -- passed from Python enforces the mutual exclusivity.
        CASE
            {%- for c in cases %}
            WHEN {{ c.condition }} THEN '{{ c.template }}'
            {%- endfor %}
            ELSE NULL
        END AS template_id
    FROM {{ eligibility_table }} c
) AS assigned
-- Only include records that were successfully claimed by a template.
WHERE template_id IS NOT NULL
{%- if unique_on %}
-- This QUALIFY clause handles deduplication efficiently in the database.
-- It selects only the first row for each unique entity, based on a deterministic order.
QUALIFY ROW_NUMBER() OVER (PARTITION BY {{ unique_on|join(', ') }} ORDER BY template_id) = 1
{%- endif %};sql/templates/waterfall_full.sql.j2::5925
-- Waterfall Report (Full / BA-level)
-- ---------------------------------------------------------------------------
-- This Jinja2 template builds a standard 5-metric waterfall for a *linear* list
-- of check columns.  It *deduplicates* the underlying eligibility table on the
-- supplied unique identifiers so that each identifier contributes **at most**
-- one row to the statistics.  The preferred row is chosen by the following
-- heuristic which mirrors the business specification in temp/logic.txt:
--   1.  Row with the *highest* number of passed checks (`pass_cnt`).
--   2.  (Tie-break) Row appearing *first* in physical order (deterministic).
--
-- Context variables (supplied by WaterfallEngine):
--   eligibility_table   : name of smart eligibility table (aliased as c)
--   unique_identifiers  : list[str] – fully-qualified identifier columns
--   check_columns       : ordered list[str] of 1/0 flag columns to evaluate
--   pre_filter          : OPTIONAL SQL predicate applied prior to stats
--   bucketable_condition: OPTIONAL predicate used only by the Regain metric
-- ---------------------------------------------------------------------------

{# Build a helper list of ID column aliases without table prefix #}
{% set uid_cols = [] %}
{% for _uid in unique_identifiers %}
    {% set _ = uid_cols.append(_uid.split('.')[-1]) %}
{% endfor %}
{% if aux_columns is not defined %}{% set aux_columns = [] %}{% endif %}
{% set all_cols = check_columns + (aux_columns if aux_columns else []) %}
{% set check_sum = ' + '.join(check_columns) if check_columns else '0' %}

-- TODO: see if I can replace * with the same column selection as the CTE flags table below for efficiency
WITH base_population AS (
    SELECT *
    FROM {{ eligibility_table }} c
    {%- if pre_filter %}
    WHERE {{ pre_filter }}
    {%- endif %}
),

-- Deduplicate so each identifier group contributes only once.  Preference is
-- given to rows with more passed checks (pass_cnt desc).
deduped AS (
    SELECT *
    FROM (
        SELECT
            bp.*,
            {{ check_sum }} AS pass_cnt,
            -- Longest unbroken streak of passes from the left
            (
                {% for i in range(check_columns|length) %}
                    {%- for j in range(i+1) %}{{ check_columns[j] }}{% if j < i %} * {% endif %}{% endfor %}
                    {%- if not loop.last %} + {% endif %}
                {% endfor %}
            ) AS streak_len,
            ROW_NUMBER() OVER (
                PARTITION BY {{ uid_cols | join(', ') }}
                ORDER BY pass_cnt DESC, streak_len DESC
            ) AS _rn
        FROM base_population bp
    ) x
    WHERE _rn = 1
),

flags AS (
    SELECT
        {% for uid in uid_cols %}{{ uid }}{{ "," if not loop.last }}{% endfor %}
        {%- if all_cols %}, {% endif %}
        {% for col in all_cols %}{{ col }}{{ "," if not loop.last }}{% endfor %}
    FROM deduped
)

-- ---------------------------------------------------------------------------
-- Metrics
-- ---------------------------------------------------------------------------
SELECT
    stat_name,
    check_name,
    cntr
FROM (
    -- Starting population (row 0)
    SELECT CAST('initial_population' AS VARCHAR(50))    AS stat_name,
           CAST('Total' AS VARCHAR(50))                 AS check_name,
           COUNT(*)                                     AS cntr
    FROM flags

    UNION ALL

    -- ①  Drop If Only This Scrub (unique_drops)
    {% for col in check_columns %}
    SELECT 'unique_drops' AS stat_name,
           '{{ col }}'    AS check_name,
           SUM(CASE WHEN {{ col }} = 0
                    {% if check_columns|length > 1 %}AND ( {%- for other in check_columns if other != col %}{{ other }} = 1{%- if not loop.last %} AND {% endif %}{% endfor %} ){% endif %}
               THEN 1 ELSE 0 END) AS cntr
    FROM flags
    {% if not loop.last %}UNION ALL{% endif %}
    {% endfor %}

    UNION ALL

    -- ②  Drop Incremental (incremental_drops)
    {% for col in check_columns %}
    SELECT 'incremental_drops' AS stat_name,
           '{{ col }}'        AS check_name,
           SUM(CASE WHEN {{ col }} = 0
                    {%- for prev in check_columns[:loop.index0] %} AND {{ prev }} = 1{%- endfor %}
               THEN 1 ELSE 0 END) AS cntr
    FROM flags
    {% if not loop.last %}UNION ALL{% endif %}
    {% endfor %}

    UNION ALL

    -- ③  Drop Cumulative (cumulative_drops)
    {% for col in check_columns %}
    SELECT 'cumulative_drops' AS stat_name,
           '{{ col }}'       AS check_name,
           COUNT(*) - SUM(CASE WHEN {{ col }} = 1
                              {%- for prev in check_columns[:loop.index0] %} AND {{ prev }} = 1{%- endfor %}
                         THEN 1 ELSE 0 END) AS cntr
    FROM flags
    {% if not loop.last %}UNION ALL{% endif %}
    {% endfor %}

    UNION ALL

    -- ④  Regain If No Scrub (regain)
    {% for col in check_columns %}
    SELECT 'regain' AS stat_name,
           '{{ col }}' AS check_name,
           SUM(CASE WHEN {{ col }} = 0
                    {% if check_columns|length > 1 %}AND ( {%- for other in check_columns if other != col %}{{ other }} = 1{%- if not loop.last %} AND {% endif %}{% endfor %} ){% endif %}
                    {%- if bucketable_condition %} AND ( {{ bucketable_condition | replace('c.', '') }} ){% endif %}
               THEN 1 ELSE 0 END) AS cntr
    FROM flags
    {% if not loop.last %}UNION ALL{% endif %}
    {% endfor %}

    UNION ALL

    -- ⑤  Remaining Population (remaining)
    {% for col in check_columns %}
    SELECT 'remaining' AS stat_name,
           '{{ col }}' AS check_name,
           SUM(CASE WHEN {{ col }} = 1
                    {%- for prev in check_columns[:loop.index0] %} AND {{ prev }} = 1{%- endfor %}
               THEN 1 ELSE 0 END) AS cntr
    FROM flags
    {% if not loop.last %}UNION ALL{% endif %}
    {% endfor %}
) AS metrics;
sql/templates/waterfall_segments.sql.j2::7353
-- Waterfall Report – Non-BA Segments (claim & exclude logic)
-- ---------------------------------------------------------------------------
--  Requirements distilled from temp/logic.txt
--  -------------------------------------------------------------
--  • Process the ordered list of *segments* for a channel.
--  • For each segment N
--        – Candidate pool   = population that has NOT been bucketed by a
--                              previous segment (N-1, N-2, …).
--        – Claimed rows     = subset of candidate pool that passes the
--                              segment’s summary condition.
--        – Produce 6 rows of output:
--              0. section="<seg name>", stat_name='Records Claimed'
--              1-5 waterfall metrics (unique_drops, incremental_drops,
--                 cumulative_drops, regain, remaining) evaluated **on the
--                 candidate pool**, not on claimed rows.
--
--  Context provided by WaterfallEngine
--  -------------------------------------------------------------
--    eligibility_table   – smart table (aliased as c in conditions)
--    unique_identifiers  – list[str]  id columns (possibly qualified)
--    pre_filter          – SQL predicate satisfied by *every* row that
--                          reaches this template (main BA + channel BA)
--    segments            – ordered list of dicts
--                            name           – display label
--                            checks         – list[str] flag column names
--                            summary_column – SQL predicate defining the
--                                            bucket for the segment
-- ---------------------------------------------------------------------------

{# Strip aliases from UID list for later convenience #}
{% set uid_cols = [] %}
{% for _uid in unique_identifiers %}
    {% set _ = uid_cols.append(_uid.split('.')[-1]) %}
{% endfor %}

WITH base_population AS (
    SELECT *
    FROM {{ eligibility_table }} AS c         -- ▼ added AS
    WHERE {{ pre_filter }}
),

-- ---------------------------------------------------------------------------
-- Deduplication (same tie-break rules as full waterfall)
-- ---------------------------------------------------------------------------

{% set all_seg_checks = [] %}
{% for seg in segments %}
    {% for chk in seg.checks %}{% set _ = all_seg_checks.append(chk) %}{% endfor %}
{% endfor %}

deduped AS (
    SELECT * FROM (
        SELECT bp.*,
               -- total passes in *all* non-BA checks (used for ranking)
               {% for chk in all_seg_checks %}{{ chk }}{%- if not loop.last %}+{% endif %}{% endfor %} AS pass_cnt,   -- ▼ opening tag no “-”
               -- longest left-to-right streak of passes across all checks
               (
                   {% for i in range(all_seg_checks|length) %}
                       {%- for j in range(i+1) %}{{ all_seg_checks[j] }}{% if j < i %} * {% endif %}{% endfor %}
                       {%- if not loop.last %} + {% endif %}
                   {% endfor %}
               ) AS streak_len,
               ROW_NUMBER() OVER (
                   PARTITION BY {{ uid_cols | join(', ') }}
                   ORDER BY pass_cnt DESC, streak_len DESC
               ) AS _rn
        FROM base_population AS bp            -- ▼ added AS
    ) t
    WHERE _rn = 1
),

-- ---------------------------------------------------------------------------
-- Build per-segment candidate & claimed CTEs
-- ---------------------------------------------------------------------------

{% for seg in segments %}
{% set idx = loop.index0 %}

seg_pool_{{ idx }} AS (
    SELECT *
    FROM deduped AS d                          -- ▼ added AS
    {# Exclude anything claimed by earlier segments #}
    {% if idx > 0 %}
    WHERE {% for prev in segments[:idx] %}
              NOT ( {{ prev.summary_column | replace('c.', '') }} )
              {% if not loop.last %}AND{% endif %}
          {% endfor %}
    {% endif %}
),

seg_claim_{{ idx }} AS (
    SELECT *
    FROM seg_pool_{{ idx }} AS sp              -- ▼ added AS
    WHERE {{ seg.summary_column | replace('c.', '') }}
){% if not loop.last %},
{% endif %}

{% endfor %}

-- ---------------------------------------------------------------------------
-- Final SELECT block – emit claimed size + 5-metric waterfall per segment
-- ---------------------------------------------------------------------------

{% for seg in segments %}
{% set idx = loop.index0 %}
{% set check_cols = seg.checks %}

-- >>> Segment: {{ seg.name }} ------------------------------------------------
-- Row 0 – records claimed by the segment
SELECT CAST('{{ seg.name }}' AS VARCHAR(50))    AS section,
       CAST('Records Claimed' AS VARCHAR(50))   AS stat_name,
       CAST('' AS VARCHAR(50))                  AS check_name,
       COUNT(*)                                 AS cntr
FROM seg_claim_{{ idx }}

UNION ALL

-- Metrics evaluated on the *candidate* pool (seg_pool)
SELECT section,
       stat_name,
       check_name,
       cntr
FROM (
    {% for metric in ['unique_drops','incremental_drops','cumulative_drops','regain','remaining'] %}
    {% for col in check_cols %}
    {% set col_idx = loop.index0 %}
    SELECT '{{ seg.name }}'                             AS section,
           '{{ metric }}'                              AS stat_name,
           '{{ col }}'                                 AS check_name,
           SUM(
               CASE
                   {% if metric == 'unique_drops' %}
                       WHEN {{ col }} = 0
                            {%- if check_cols|length > 1 %} AND ({% for other in check_cols if other != col %} {{ other }} = 1 {% if not loop.last %}AND{% endif %}{% endfor %}){% endif %}
                       THEN 1
                   {% elif metric == 'incremental_drops' %}
                       WHEN {{ col }} = 0
                            {%- for prev in check_cols[:col_idx] %} AND {{ prev }} = 1{% endfor %}
                       THEN 1
                   {% elif metric == 'cumulative_drops' %}
                       WHEN NOT ( {% for prev in check_cols[:col_idx+1] %} {{ prev }} = 1 {% if not loop.last %}AND{% endif %}{% endfor %} )
                       THEN 1
                   {% elif metric == 'regain' %}
                       WHEN {{ col }} = 0
                            {%- for other in check_cols if other != col %} AND {{ other }} = 1{% endfor %}
                       THEN 1
                   {% elif metric == 'remaining' %}
                       WHEN {% for prev in check_cols[:col_idx+1] %} {{ prev }} = 1 {% if not loop.last %}AND{% endif %}{% endfor %}
                       THEN 1
                   {% endif %}
                   ELSE 0
               END
           ) AS cntr
    FROM (
        SELECT {% for uid in uid_cols %}{{ uid }}{{ "," if not loop.last }}{% endfor %}
               {%- if check_cols %}, {% endif %}
               {% for chk in check_cols %}{{ chk }}{{ "," if not loop.last }}{% endfor %}
        FROM seg_pool_{{ idx }}
    ) AS flags({{ (uid_cols + check_cols)|join(', ') }})
    {% if not (metric == 'remaining' and col_idx == check_cols|length -1) %}
    UNION ALL
    {% endif %}
    {% endfor %}
    {% endfor %}
) AS metrics

{% if not loop.last %}UNION ALL{% endif %}

{% endfor %}
;
engines/output.py::8473
"""
Output engine: exports final data to files with optional transforms.
"""
from tlptaco.config.schema import OutputConfig
from tlptaco.db.runner import DBRunner
from tlptaco.utils.logging import get_logger
import tlptaco.iostream.writer as io_writer
from tlptaco.sql.generator import SQLGenerator
import os
import importlib


class OutputEngine:
    def __init__(self, cfg: OutputConfig, runner: DBRunner, logger=None):
        self.cfg = cfg
        self.runner = runner
        self.logger = logger or get_logger("output")
        # Cache for prepared jobs and the eligibility engine
        self._output_jobs = None
        self._eligibility_engine = None

    def _prepare_output_steps(self, eligibility_engine):
        """
        Prepares all output jobs, including SQL generation, without executing them.
        The results are cached to avoid redundant work.
        """
        self._eligibility_engine = eligibility_engine

        if self._output_jobs is not None:
            self.logger.info("Using cached output steps.")
            return

        self.logger.info("No cached steps found. Preparing output jobs and SQL.")
        self._output_jobs = []
        elig_cfg = eligibility_engine.cfg
        templates_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'sql', 'templates'))
        gen = SQLGenerator(templates_dir)

        # --- START MODIFICATION ---
        def create_sql_condition(check_list):
            """Helper function to create a combined SQL AND condition."""
            if not check_list:
                return "1 = 1"  # Return a tautology if the list is empty
            return " AND ".join([f"c.{check.name} = 1" for check in check_list])

        for channel_name, out_cfg in self.cfg.channels.items():
            self.logger.info(f"Preparing logic for channel '{channel_name}'")

            if channel_name not in elig_cfg.conditions.channels:
                self.logger.warning(f"Channel '{channel_name}' in output config but not eligibility. Skipping.")
                continue

            channel_elig_cfg = elig_cfg.conditions.channels[channel_name]
            cases = []
            exclusion_conditions = []

            # Condition for main BA checks
            main_ba_condition = create_sql_condition(elig_cfg.conditions.main.BA)

            # Case 1: Channel BA
            channel_ba_checks = channel_elig_cfg.BA
            channel_ba_condition = create_sql_condition(channel_ba_checks)
            cases.append({
                'template': f'{channel_name}_BA',
                'condition': f"({main_ba_condition}) AND ({channel_ba_condition})"
            })

            # Case 2: Other segments (sorted by priority as defined in YAML)
            if channel_elig_cfg.others:
                for segment_name, segment_checks in channel_elig_cfg.others.items():
                    segment_condition = create_sql_condition(segment_checks)

                    # Conditions for this segment include main BA, channel BA, and the segment's own checks
                    current_conditions = [
                        f"({main_ba_condition})",
                        f"({channel_ba_condition})",
                        f"({segment_condition})",
                        *exclusion_conditions
                    ]
                    cases.append({
                        'template': f'{channel_name}_{segment_name}',
                        'condition': " AND ".join(current_conditions)
                    })

                    # Add the inverse of this segment's condition to the exclusion list for the *next* segment
                    # This ensures mutual exclusivity
                    inverse_segment_condition = " OR ".join([f"c.{check.name} = 0" for check in segment_checks])
                    if inverse_segment_condition:
                        exclusion_conditions.append(f"({inverse_segment_condition})")
            # --- END MODIFICATION ---

            context = {'eligibility_table': elig_cfg.eligibility_table, 'columns': out_cfg.columns,
                       'unique_on': out_cfg.unique_on, 'cases': cases}
            sql = gen.render('output.sql.j2', context)

            # Log rendered SQL
            from tlptaco.utils.logging import log_sql_section
            log_sql_section(f'Output - {channel_name}', sql)

            # Determine file extension: use .xlsx for 'excel'
            fmt = out_cfg.output_options.format.lower()

            if fmt == 'table':
                # file_location = schema, file_base_name = table
                full_table = f"{out_cfg.file_location}.{out_cfg.file_base_name}"
                self._output_jobs.append({
                    'channel_name': channel_name,
                    'sql': sql,
                    'fmt': 'table',
                    'table_name': full_table,
                    'unique_on': out_cfg.unique_on,
                })
            else:
                ext = 'xlsx' if fmt == 'excel' else fmt
                path = os.path.join(out_cfg.file_location,
                                    f"{out_cfg.file_base_name}.{ext}")
                self._output_jobs.append({
                    'channel_name': channel_name,
                    'sql': sql,
                    'fmt': fmt,
                    'path': path,
                    'output_options': out_cfg.output_options
                })

    def num_steps(self, eligibility_engine) -> int:
        """
        Calculates the total number of output files to be generated.
        Caches the eligibility_engine for the run() method.
        """
        self.logger.info("Calculating the number of output steps.")
        self._prepare_output_steps(eligibility_engine)
        total_steps = len(self._output_jobs)
        self.logger.info(f"Calculation complete: {total_steps} steps (files).")
        return total_steps

    def run(self, eligibility_engine=None, progress=None):
        """
        For each channel, runs the SQL and writes the final output file.
        The eligibility_engine is optional if already provided to num_steps().
        """
        engine_to_use = eligibility_engine or self._eligibility_engine
        if not engine_to_use:
            raise ValueError(
                "An eligibility_engine instance must be provided either to run() or to a prior num_steps() call.")

        self._prepare_output_steps(engine_to_use)

        for job in self._output_jobs:
            channel_name = job['channel_name']
            self.logger.info(f"Running output job for channel {channel_name}")
            self.logger.debug(job['sql'])

            if job['fmt'] == 'table':
                table_full = job['table_name']
                try:
                    self.logger.info(f"Dropping existing table {table_full} (if any)")
                    self.runner.run(f"DROP TABLE {table_full};")
                except Exception:
                    self.logger.debug("No pre-existing table to drop or driver raised warning")

                create_sql = (
                    f"CREATE MULTISET TABLE {table_full} AS (\n" +
                    job['sql'].rstrip().rstrip(';') +
                    "\n) WITH DATA;"
                )
                self.logger.info(f"Creating output table {table_full}")
                self.runner.run(create_sql)
            else:
                df = self.runner.to_df(job['sql'])
                self.logger.info(f"Fetched {len(df)} rows for channel {channel_name}")

                cf = job['output_options'].custom_function if 'output_options' in job else None
                if cf:
                    module_name, fn_name = cf.rsplit('.', 1)
                    mod = importlib.import_module(module_name)
                    func = getattr(mod, fn_name)
                    self.logger.info(f"Applying custom function {fn_name} to channel {channel_name}")
                    df = func(df)

                os.makedirs(os.path.dirname(job['path']), exist_ok=True)
                self.logger.info(f"Writing output file for channel {channel_name} to {job['path']}")
                # Delegate to io_writer.write_dataframe so tests can monkey-patch
                io_writer.write_dataframe(
                    df,
                    job['path'],
                    job['output_options'].format,
                    **(job['output_options'].additional_arguments or {})
                )

            if progress:
                progress.update("Output")engines/waterfall.py::25624
"""
Waterfall engine: computes waterfall metrics from the smart eligibility table.
"""
from tlptaco.config.schema import WaterfallConfig, EligibilityConfig
from tlptaco.db.runner import DBRunner
from tlptaco.utils.logging import get_logger
from tlptaco.sql.generator import SQLGenerator
import os
import pandas as pd
from datetime import datetime


class WaterfallEngine:
    def __init__(self, cfg: WaterfallConfig, runner: DBRunner, logger=None):
        self.cfg = cfg
        self.runner = runner
        self.logger = logger or get_logger("waterfall")
        # Metadata (to be set by CLI)
        self.offer_code: str = ''
        self.campaign_planner: str = ''
        self.lead: str = ''
        # Cache for prepared steps and the eligibility engine
        self._waterfall_groups = None
        self._eligibility_engine = None

    def _prepare_waterfall_steps(self, eligibility_engine):
        """
        Prepares all the groups and SQL generation steps without executing them.
        The results are cached to avoid redundant work.
        """
        self._eligibility_engine = eligibility_engine

        if self._waterfall_groups is not None:
            self.logger.info("Using cached waterfall steps.")
            return

        self.logger.info("No cached steps found. Preparing waterfall groups and SQL.")
        self._waterfall_groups = []
        elig_cfg: EligibilityConfig = eligibility_engine.cfg

        # 1. Determine the grouping columns
        groups = []
        for item in self.cfg.count_columns:
            raw_cols = [item] if isinstance(item, str) else list(item)
            grp_name = '_'.join([col.split('.')[-1] for col in raw_cols])
            cols = [f"c.{col.split('.')[-1]}" for col in raw_cols]
            groups.append({'name': grp_name, 'cols': cols})

        templates_dir = os.path.join(os.path.dirname(__file__), '..', 'sql', 'templates')
        gen = SQLGenerator(templates_dir)

        def create_sql_condition(check_list, operator='AND'):
            """Helper function to create a combined SQL condition."""
            if not check_list:
                return "1=1"
            op = f" {operator.strip()} "
            conditions = [f"c.{check.name} = 1" for check in check_list]
            return f"({op.join(conditions)})"

        # 2. For each group, prepare the SQL and metadata for each report section
        for grp in groups:
            name, uniq_ids = grp['name'], grp['cols']
            sql_jobs = []

            # --- SECTION 1: MAIN/BASE WATERFALL ---
            main_ba_checks = [chk.name for chk in elig_cfg.conditions.main.BA]
            # Base waterfall (main BA) has no bucketable filter
            ctx_main = {
                'eligibility_table': elig_cfg.eligibility_table,
                'unique_identifiers': uniq_ids,
                'check_columns': main_ba_checks,
                'aux_columns': [],
                'pre_filter': None,
                'segments': [],  # not used in full template
                'bucketable_condition': None
            }

            sql_main = gen.render('waterfall_full.sql.j2', ctx_main)
            from tlptaco.utils.logging import log_sql_section
            log_sql_section(f'Waterfall {name} - Base', sql_main)
            sql_jobs.append({'type': 'standard', 'sql': sql_main, 'section_name': 'Base'})

            # --- SECTION 2: PER-CHANNEL WATERFALLS ---
            for channel_name, channel_cfg in elig_cfg.conditions.channels.items():
                # Prepare per-channel non-BA segments list for Regain logic
                segments_to_process = []
                # Base filter: passed all main BA checks
                base_filter = create_sql_condition(elig_cfg.conditions.main.BA)

                # Channel BA checks
                channel_ba_checks_list = channel_cfg.BA
                channel_ba_check_names = [chk.name for chk in channel_ba_checks_list]
                # CHANNEL BA WATERFALL
                if channel_ba_check_names:
                    # Build OR-list of segment summary conditions for bucketable filter
                    if channel_cfg.others:
                        # Preserve original segment order as defined in YAML
                        seg_conds = [create_sql_condition(s_checks) for _, s_checks in channel_cfg.others.items()]
                        bucketable = ' OR '.join([f'({c})' for c in seg_conds])

                        # Collect additional flag columns referenced by bucketable filter
                        aux_cols: list[str] = []
                        for _, s_checks in channel_cfg.others.items():
                            aux_cols.extend([chk.name for chk in s_checks])
                        aux_cols = [c for c in aux_cols if c not in channel_ba_check_names]
                    else:
                        bucketable = None
                        aux_cols = []

                    ctx_chan_ba = {
                        'eligibility_table': elig_cfg.eligibility_table,
                        'unique_identifiers': uniq_ids,
                        'check_columns': channel_ba_check_names,
                        'aux_columns': aux_cols,
                        'pre_filter': base_filter,
                        'segments': segments_to_process,
                        'bucketable_condition': bucketable
                    }
                    sql_chan_ba = gen.render('waterfall_full.sql.j2', ctx_chan_ba)
                    sql_jobs.append({'type': 'standard', 'sql': sql_chan_ba, 'section_name': f'{channel_name} - BA'})
                    log_sql_section(f'Waterfall {name} - {channel_name} BA', sql_chan_ba)

                # CHANNEL non-BA segments
                if channel_cfg.others:
                    channel_ba_condition = create_sql_condition(channel_ba_checks_list)
                    segment_base_filter = f"{base_filter} AND {channel_ba_condition}"

                    # For each non-BA segment, prepare summary and detailed SQL
                    for s_name, s_checks in channel_cfg.others.items():
                        # Summary condition: pass all checks in this segment
                        segment_condition = create_sql_condition(s_checks)
                        segments_to_process.append({
                            'name': f'{channel_name} - {s_name}',
                            'checks': [c.name for c in s_checks],
                            'summary_column': segment_condition
                        })

                    ctx_segments = {
                        'eligibility_table': elig_cfg.eligibility_table,
                        'unique_identifiers': uniq_ids,
                        'pre_filter': segment_base_filter,
                        'segments': segments_to_process
                    }
                    sql_segments = gen.render('waterfall_segments.sql.j2', ctx_segments)
                    sql_jobs.append({'type': 'segments', 'sql': sql_segments})
                    log_sql_section(f'Waterfall {name} - {channel_name} Segments', sql_segments)

            out_path = os.path.join(self.cfg.output_directory,
                                    f"waterfall_report_{elig_cfg.eligibility_table}_{name}.xlsx")
            self._waterfall_groups.append({'name': name, 'jobs': sql_jobs, 'output_path': out_path})

    def num_steps(self, eligibility_engine) -> int:
        """
        Calculates the total number of waterfall reports (groups) to be generated.
        Caches the eligibility_engine for the run() method.
        """
        self.logger.info("Calculating the number of waterfall steps.")
        self._prepare_waterfall_steps(eligibility_engine)
        total_steps = len(self._waterfall_groups)
        self.logger.info(f"Calculation complete: {total_steps} steps (reports).")
        return total_steps

    def _pivot_waterfall_df(self, df, section_name):
        """Pivots the long-format waterfall data into a wide-format DataFrame."""
        # Exclude summary rows and any initial-population rows
        metric_df = df[~df['stat_name'].isin(['Records Claimed', 'initial_population'])]
        if metric_df.empty:
            return pd.DataFrame()
        pivoted = metric_df.pivot_table(index='check_name', columns='stat_name', values='cntr').reset_index()
        pivoted['section'] = section_name
        return pivoted

    def run(self, eligibility_engine=None, progress=None):
        """
        Orchestrates the waterfall report. The eligibility_engine is optional
        if it was already provided in a prior call to num_steps().
        """
        # Determine which eligibility engine to use
        engine_to_use = eligibility_engine or self._eligibility_engine

        # If no engine is available from either the argument or the cache, raise an error.
        if not engine_to_use:
            raise ValueError(
                "An eligibility_engine instance must be provided either to run() or to a prior num_steps() call.")

        self._prepare_waterfall_steps(engine_to_use)
        os.makedirs(self.cfg.output_directory, exist_ok=True)

        # Collect compiled metrics for *all* groups. Each item will be a tuple
        # (group_name, compiled_sections)
        compiled_groups: list[tuple[str, list[tuple[str, pd.DataFrame]]]] = []
        # Starting population per group (group_name -> int)
        starting_pops: dict[str, int] = {}

        # Holder for *previous* runs fetched from SQLite history so that the
        # Excel writer can render side-by-side comparison tabs.
        previous_groups: dict[str, list[tuple[str, pd.DataFrame]]] = {}

        # Pre-compute condition rows once (shared across groups)
        conds = engine_to_use.cfg.conditions
        cond_rows: list[dict] = []
        # main BA
        for chk in conds.main.BA:
            cond_rows.append({'check_name': chk.name, 'sql': chk.sql, 'description': chk.description})
        # channel BA and segments
        for chname, chcfg in conds.channels.items():
            for chk in chcfg.BA:
                cond_rows.append({'check_name': chk.name, 'sql': chk.sql, 'description': chk.description})
            for seg_checks in chcfg.segments.values():
                for chk in seg_checks:
                    cond_rows.append({'check_name': chk.name, 'sql': chk.sql, 'description': chk.description})

        # ------------------------------------------------------------------
        # Enhance conditions dataframe with Section / Template / # columns for
        # the revamped Excel layout.
        # ------------------------------------------------------------------
        import re

        def _parse_check_name(name: str):
            """Split a check name like 'email_loyalty_B_1' into (section, template, #)."""
            parts = name.split('_')
            if len(parts) < 2:
                return name, '', ''
            section = parts[0]
            # Last numeric part (if any)
            num_match = re.match(r'^(\d+)$', parts[-1])
            if num_match:
                num = int(parts[-1])
                mid = parts[1:-1]
            else:
                num = ''
                mid = parts[1:]
            template = mid[0] if mid else ''
            return section, template, num

        enriched_rows = []
        for row in cond_rows:
            sec, tpl, num = _parse_check_name(row['check_name'])
            enriched_rows.append({
                'check_name': row['check_name'],
                'Section': sec,
                'Template': tpl,
                '#': num,
                'sql': row['sql'],
                'description': row['description']
            })

        conditions_df = pd.DataFrame(enriched_rows).set_index('check_name')

        for group in self._waterfall_groups:
            all_report_sections = []
            try:
                for job in group['jobs']:
                    df_raw = self.runner.to_df(job['sql'])
                    if 'cntr' not in df_raw.columns and 'value' in df_raw.columns:
                        df_raw = df_raw.rename(columns={'value': 'cntr'})

                    if job['type'] == 'standard':
                        df_pivoted = self._pivot_waterfall_df(df_raw, job['section_name'])
                        all_report_sections.append(df_pivoted)

                        # Capture starting population if not yet stored for this group
                        if group['name'] not in starting_pops:
                            sp = df_raw.loc[df_raw['stat_name'] == 'initial_population', 'cntr']
                            if not sp.empty:
                                starting_pops[group['name']] = int(sp.iloc[0])

                    elif job['type'] == 'segments':
                        detail_rows = df_raw[df_raw['stat_name'] != 'Records Claimed'].copy()
                        for section_name in detail_rows['section'].unique():
                            section_df = self._pivot_waterfall_df(
                                detail_rows[detail_rows['section'] == section_name],
                                section_name
                            )
                            if not section_df.empty:
                                all_report_sections.append(section_df)

                if all_report_sections:
                    compiled = []
                    for df in all_report_sections:
                        sec = df['section'].iat[0]
                        section_df = df.drop(columns='section').reset_index(drop=True)
                        compiled.append((sec, section_df))
                    compiled_groups.append((group['name'], compiled))

                    # --------------------------------------------------
                    # Attempt to fetch a *previous* snapshot for this
                    # group from the history DB (if available).
                    # --------------------------------------------------
                    prev_result = self._fetch_previous_group_metrics(group['name'])
                    if prev_result:
                        prev_date, prev_compiled, prev_start_pop = prev_result
                        previous_groups[group['name']] = {
                            'date': prev_date,
                            'compiled': prev_compiled,
                            'start_pop': prev_start_pop,
                        }

            except Exception as e:
                self.logger.exception(f"Waterfall grouping '{group['name']}' failed: {e}")
            finally:
                if progress:
                    progress.update('Waterfall')

        # ------------------------------------------------------------------
        # After processing *all* groups, write a single consolidated workbook
        # ------------------------------------------------------------------
        if compiled_groups:
            import tlptaco.engines.waterfall_excel as wf_excel_mod
            # Timestamped filename using offer_code_YYYY_MM_DD_HH:MM:SS.xlsx
            timestamp_str = datetime.now().strftime("%Y_%m_%d_%H:%M:%S")

            # Sanitize offer code for filesystem safety (letters, numbers, _ -)
            import re
            safe_offer = re.sub(r'[^A-Za-z0-9_-]+', '_', self.offer_code or 'run')

            file_name = f"{safe_offer}_{timestamp_str}.xlsx"

            out_path = os.path.join(self.cfg.output_directory, file_name)

            wf_excel_mod.write_waterfall_excel(
                conditions_df,
                compiled_groups,
                out_path,
                previous=previous_groups,
                offer_code=self.offer_code,
                campaign_planner=self.campaign_planner,
                lead=self.lead,
                current_date=timestamp_str.replace('_', '-') ,
                starting_pops=starting_pops,
            )
            self.logger.info(f"Consolidated waterfall report written to {out_path}")

            # ------------------------------------------------------------------
            # Persist results to history database if enabled in configuration
            # ------------------------------------------------------------------
            try:
                self._log_history(conditions_df, compiled_groups)
            except Exception:
                # History logging should never crash the main pipeline – log & continue
                self.logger.exception("Failed to write waterfall history")

    # ------------------------------------------------------------------
    # Private helpers
    # ------------------------------------------------------------------

    def _get_history_db_path(self) -> str:
        """Return absolute path to the SQLite history DB based on config."""
        hist_cfg = self.cfg.history
        if hist_cfg.db_path:
            return os.path.abspath(hist_cfg.db_path)
        # default inside the output directory
        return os.path.join(self.cfg.output_directory, 'waterfall_history.sqlite')

    def _log_history(self, conditions_df, compiled_groups):
        """Insert results of this run into a SQLite history table.

        Parameters
        ----------
        conditions_df : pandas.DataFrame
            DataFrame indexed by check_name containing `sql` and `description`.
        compiled_groups : list[tuple[str, list[tuple[str, pandas.DataFrame]]]]
            Output structure from WaterfallEngine containing metrics per group.
        """
        # Guard clause – skip if tracking disabled
        if not self.cfg.history.track:
            return

        import sqlite3
        from datetime import datetime as _dt

        db_path = self._get_history_db_path()
        # Ensure parent directory exists
        os.makedirs(os.path.dirname(db_path), exist_ok=True)

        conn = sqlite3.connect(db_path)
        cur = conn.cursor()

        cur.execute(
            """
            CREATE TABLE IF NOT EXISTS waterfall_history (
                run_datetime      TEXT,
                group_name        TEXT,
                check_name        TEXT,
                criteria          TEXT,
                description       TEXT,
                unique_drops      INTEGER,
                regain            INTEGER,
                incremental_drops INTEGER,
                cumulative_drops  INTEGER,
                remaining         INTEGER
            );
            """
        )

        run_dt = _dt.now().isoformat(timespec='seconds')

        metric_cols = [
            'unique_drops',
            'regain',
            'incremental_drops',
            'cumulative_drops',
            'remaining',
        ]

        insert_sql = (
            "INSERT INTO waterfall_history (run_datetime, group_name, check_name, "
            "criteria, description, unique_drops, regain, incremental_drops, cumulative_drops, remaining) "
            "VALUES (?,?,?,?,?,?,?,?,?,?)"
        )

        for group_name, compiled in compiled_groups:
            for _, df in compiled:
                for _, row in df.iterrows():
                    check_name = row.get('check_name')
                    try:
                        crit = conditions_df.loc[check_name, 'sql']
                        desc = conditions_df.loc[check_name, 'description']
                    except Exception:
                        crit = None
                        desc = None

                    metrics = [row.get(col) if col in row else None for col in metric_cols]

                    cur.execute(
                        insert_sql,
                        (
                            run_dt,
                            group_name,
                            check_name,
                            crit,
                            desc,
                            *metrics,
                        ),
                    )

        conn.commit()
        conn.close()
        self.logger.info(f"Waterfall run history appended to {db_path}")

    # ------------------------------------------------------------------
    # History *read* helper – fetch latest snapshot for a group
    # ------------------------------------------------------------------

    def _fetch_previous_group_metrics(self, group_name: str):
        """Return the most recent waterfall metrics for *group_name* within
        the configured look-back window.

        Returns
        -------
        list[tuple[str, pandas.DataFrame]] | None
            A list in the same structure as the *compiled* argument used by
            the Excel writer: ``[(section_name, df), ...]``.  ``None`` if no
            history rows are available.
        """
        # Ensure history DB path exists
        db_path = self._get_history_db_path()
        if not os.path.isfile(db_path):
            return None

        import sqlite3
        import pandas as pd
        from datetime import datetime as _dt, timedelta as _td

        # ------------------------------------------------------------------
        # Determine selection strategy: legacy *lookback_days* vs the new
        # *days_ago_to_compare* parameter.  When the latter is provided we
        # ignore look-back logic and instead fetch **all** rows for the group
        # (we will pick the snapshot closest to the target point-in-time in
        # Python).  This avoids overly complex SQL and keeps the behaviour
        # deterministic even if the window is wider than the history range.
        # ------------------------------------------------------------------

        days_ago = self.cfg.history.compare_offset_days

        conn = sqlite3.connect(db_path)
        prev_start_pop: int | None = None
        try:
            if days_ago is not None:
                # Fetch *all* rows for this group – volume is expected to be
                # small (one row per check per historic run).
                query = (
                    "SELECT * FROM waterfall_history "
                    "WHERE group_name = ?"
                )
                df_raw = pd.read_sql(query, conn, params=(group_name,))
            else:
                # Legacy behaviour: windowed look-back then pick newest.
                lookback_days = self.cfg.history.recent_window_days or 30
                query = (
                    "SELECT * FROM waterfall_history "
                    "WHERE group_name = ? "
                    "AND run_datetime >= datetime('now', ?) "
                    "ORDER BY run_datetime DESC"
                )
                offset = f'-{int(lookback_days)} days'
                df_raw = pd.read_sql(query, conn, params=(group_name, offset))
        except Exception as ex:
            self.logger.debug(f"Unable to read prior history for group '{group_name}': {ex}")
            return None
        finally:
            conn.close()

        if df_raw.empty:
            return None

        # Ensure run_datetime parsed to datetime objects for comparison logic
        df_raw['run_dt_obj'] = pd.to_datetime(df_raw['run_datetime'])

        if days_ago is not None:
            # Target date/time – midnight-ish exact time not critical because
            # we will measure absolute delta.
            target_dt = _dt.now() - _td(days=int(days_ago))

            # Compute absolute time delta (in seconds) per row then pick the
            # minimal delta *per run*, finally choose the run with smallest
            # delta overall.
            df_raw['abs_delta'] = (df_raw['run_dt_obj'] - target_dt).abs()

            # Identify the run_datetime (timestamp) with the smallest delta
            nearest_idx = df_raw['abs_delta'].idxmin()
            nearest_dt = df_raw.loc[nearest_idx, 'run_datetime']

            df_latest = df_raw[df_raw['run_datetime'] == nearest_dt].copy()
            # Determine starting population for this historic run
            if 'stat_name' in df_raw.columns:
                sp_series = df_raw[(df_raw['run_datetime'] == nearest_dt) &
                                   (df_raw['check_name'] == 'Total') &
                                   (df_raw['stat_name'] == 'initial_population')]['cntr']
                if not sp_series.empty:
                    prev_start_pop = int(sp_series.iloc[0])
        else:
            # Legacy: most recent inside window
            latest_dt = df_raw['run_datetime'].max()
            df_latest = df_raw[df_raw['run_datetime'] == latest_dt].copy()
            if 'stat_name' in df_raw.columns:
                sp_series = df_raw[(df_raw['run_datetime'] == latest_dt) &
                                   (df_raw['check_name'] == 'Total') &
                                   (df_raw['stat_name'] == 'initial_population')]['cntr']
                if not sp_series.empty:
                    prev_start_pop = int(sp_series.iloc[0])

        metric_cols = [
            'unique_drops',
            'regain',
            'incremental_drops',
            'cumulative_drops',
            'remaining',
        ]

        # Keep only relevant columns to match the pivoted structure used by
        # the writer.
        cols_available = [c for c in metric_cols if c in df_latest.columns]
        if not cols_available:
            return None

        df_wide = df_latest[['check_name', *cols_available]].copy()
        # Add a dummy 'section' column so downstream code can reuse the same
        # handling logic.  We flag it as 'Previous'.
        df_wide['section'] = 'Historical'

        # Reset index order similar to pivoting routine
        df_wide = df_wide.reset_index(drop=True)

        return (nearest_dt if days_ago is not None else latest_dt,
                [('Historical', df_wide)],
                prev_start_pop)engines/eligibility.py::5018
"""
Eligibility engine: runs work tables and eligibility SQL.
"""
from tlptaco.config.schema import EligibilityConfig
from tlptaco.db.runner import DBRunner
from tlptaco.utils.logging import get_logger
from tlptaco.sql.generator import SQLGenerator
import os

class EligibilityEngine:
    def __init__(self, cfg: EligibilityConfig, runner: DBRunner, logger=None):
        self.cfg = cfg
        self.runner = runner
        self.logger = logger or get_logger("eligibility")
        self._sql_statements = None

    def _prepare_sql(self):
        """
        Prepares the SQL statements for execution.
        This method builds the context and renders the SQL template, but only once.
        The generated statements are cached in self._sql_statements.
        """
        if self._sql_statements is not None:
            self.logger.info("Using cached SQL statements.")
            return

        self.logger.info("No cached SQL found. Generating new SQL statements.")
        cfg = self.cfg
        tables = []
        where_clauses = []
        for t in cfg.tables:
            tables.append({
                'name': t.name,
                'alias': t.alias,
                'join_type': t.join_type or '',
                'join_conditions': t.join_conditions or ''
            })
            if t.where_conditions:
                where_clauses.append(t.where_conditions)

        # --- START MODIFICATION ---
        # Gather ALL checks from the entire configuration.
        all_checks = []
        # Add main BA checks
        all_checks.extend(cfg.conditions.main.BA)
        # Add main segments checks (legacy 'others' merged into 'segments')
        for segment_checks in cfg.conditions.main.segments.values():
            all_checks.extend(segment_checks)

        # Add all channel checks
        for channel_cfg in cfg.conditions.channels.values():
            # Add channel BA checks
            all_checks.extend(channel_cfg.BA)
            # Add channel segments checks
            for segment_checks in channel_cfg.segments.values():
                all_checks.extend(segment_checks)

        # Ensure we have a list of unique checks, in case of duplicates
        # Using a dict to preserve order and uniqueness based on 'name'
        unique_checks_dict = {chk.name: chk for chk in all_checks}
        final_checks = list(unique_checks_dict.values())

        context = {
            'eligibility_table': cfg.eligibility_table,
            'unique_identifiers': cfg.unique_identifiers,
            'unique_without_aliases': [u.split('.')[-1] for u in cfg.unique_identifiers],
            'tables': tables,
            'where_clauses': where_clauses,
            'checks': [
                {'name': chk.name, 'sql': chk.sql}
                for chk in final_checks
            ],
        }
        # --- END MODIFICATION ---

        tmpl_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'sql', 'templates'))
        gen = SQLGenerator(tmpl_dir)
        sql = gen.render('eligibility.sql.j2', context)

        self._sql_statements = [stmt for stmt in sql.split(';') if stmt.strip()]

        # Log rendered SQL for visibility/copy-paste
        from tlptaco.utils.logging import log_sql_section
        log_sql_section('Eligibility', sql)

    def num_steps(self) -> int:
        """
        Calculates the total number of SQL statements that will be executed by the run() method.
        Uses the cached SQL if available.

        Returns:
            int: The total number of steps (SQL statements).
        """
        self.logger.info("Calculating the number of steps for the eligibility run.")
        # Ensure the SQL statements are prepared and cached
        self._prepare_sql()

        # The total number of steps is the count of main statements + 1 for the DROP TABLE command.
        total_steps = 1 + len(self._sql_statements)

        self.logger.info(f"Calculation complete: {total_steps} steps.")
        return total_steps

    def run(self, progress=None):
        """
        Render and execute eligibility SQL to create a "smart" table.
        Uses cached SQL if num_steps() was called first.

        Args:
            progress: An optional progress tracking object with an update() method.
        """
        # Ensure the SQL statements are prepared and cached
        self._prepare_sql()

        # Step 1: Attempt to drop the existing table.
        try:
            self.logger.info(f"Dropping existing table {self.cfg.eligibility_table}")
            self.runner.run(f"DROP TABLE {self.cfg.eligibility_table};")
        except Exception:
            self.logger.info("No existing eligibility table to drop")

        if progress:
            progress.update("Eligibility")

        # Step 2: Execute each of the cached statements.
        for stmt in self._sql_statements:
            self.logger.info('Executing eligibility SQL statement')
            self.runner.run(stmt)
            if progress:
                progress.update("Eligibility")
engines/waterfall_excel.py::21807
"""
Excel writer for waterfall reports (current + history).

The workbook layout is:

1. Sheet "waterfall" – consolidated report identical to the original format
   (all groups side-by-side).
2. One additional sheet *per group* containing:
      • a full table for the *current* run
      • a full table for the *previous* run within the configured look-back
        window (if no prior data → note displayed instead).

This module has **no legacy compatibility shims** – it is only used by
WaterfallEngine v2 and tests inside this repository.
"""

from __future__ import annotations

from typing import List, Tuple, Dict, Iterable, Any

import pandas as pd
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment

# ────────────────────────────────────────────────────────────────────────────────
# Constants / helper data -------------------------------------------------------
# ────────────────────────────────────────────────────────────────────────────────


_BASE_TITLES = ["Section", "Template", "#", "Criteria", "Description"]

# Column order & friendly display names for metrics
_METRIC_ORDER: List[Tuple[str, str]] = [
    ("unique_drops", "Drop If Only This Scrub"),
    ("regain", "Regain If No Scrub"),
    ("incremental_drops", "Drop Incremental"),
    ("cumulative_drops", "Drop Cumulative"),
    ("remaining", "Remaining"),
]

_HEADER_FILL = PatternFill(start_color="E0E0E0", end_color="E0E0E0", fill_type="solid")

# Additional palette colours
_BLOCK_HEADER_FILL = PatternFill(start_color="6495ED", end_color="6495ED", fill_type="solid")  # Cornflower Blue
_HIST_ONLY_FILL   = PatternFill(start_color="FFF9C4", end_color="FFF9C4", fill_type="solid")  # Light Yellow
_CUR_ONLY_FILL    = PatternFill(start_color="C8E6C9", end_color="C8E6C9", fill_type="solid")  # Light Mint


# ────────────────────────────────────────────────────────────────────────────────
# Public API --------------------------------------------------------------------
# ────────────────────────────────────────────────────────────────────────────────


def write_waterfall_excel(
    conditions: pd.DataFrame,
    compiled_current: List[Tuple[str, List[Tuple[str, pd.DataFrame]]]],
    output_path: str,
    *,
    previous: Dict[str, Dict[str, Any]] | None = None,
    offer_code: str = "",
    campaign_planner: str = "",
    lead: str = "",
    current_date: str = "",
    starting_pops: Dict[str, int] | None = None,
) -> None:
    """Write the Excel workbook for a Waterfall run.

    Parameters
    ----------
    conditions
        DataFrame indexed by *check_name* with extra descriptive columns.
    compiled_current
        Output of WaterfallEngine for the current run: list of
        ``(group_name, compiled)`` where *compiled* == list of
        ``(section_name, pivoted_df)``.
    output_path
        Destination ``.xlsx`` file.
    previous
        Optional mapping: ``{group_name: compiled_previous}`` following the
        same internal *compiled* structure. Used to render comparison tables
        on individual group sheets.
    starting_pops
        Mapping ``group_name -> starting_population``. If omitted the
        *Starting Population* row is left blank.
    """

    if previous is None:
        previous = {}
    if starting_pops is None:
        starting_pops = {}

    wb = Workbook()
    ws_cons = wb.active
    ws_cons.title = "waterfall"

    _write_header(ws_cons, offer_code, campaign_planner, lead, current_date)
    _write_consolidated_table(ws_cons, conditions, compiled_current, starting_pops)

    # ------------------------------------------------------------------
    # Per-group sheets ---------------------------------------------------
    # ------------------------------------------------------------------
    for group_name, compiled_cur in compiled_current:
        sheet_name = group_name[:31]
        ws_grp = wb.create_sheet(title=sheet_name)

        _write_header(ws_grp, offer_code, campaign_planner, lead, current_date)

        prev_info = previous.get(group_name) if previous else None

        if prev_info:
            hist_date = prev_info.get('date', '')
            compiled_prev = prev_info['compiled']
            prev_start_pop = prev_info.get('start_pop')

            # Comparison table side-by-side
            _write_group_comparison_table(
                ws_grp,
                start_row=2,
                group_name=group_name,
                compiled_current=compiled_cur,
                compiled_historic=compiled_prev,
                conditions=conditions,
                start_pop_current=starting_pops.get(group_name),
                start_pop_historic=prev_start_pop,
                historic_date=hist_date,
            )
        else:
            # Fall back to legacy single table view
            _write_group_table(
                ws_grp,
                start_row=2,
                group_name=group_name,
                compiled=compiled_cur,
                conditions=conditions,
                start_pop=starting_pops.get(group_name),
            )

    wb.save(output_path)


# ────────────────────────────────────────────────────────────────────────────────
# Private helpers ----------------------------------------------------------------
# ────────────────────────────────────────────────────────────────────────────────


def _write_header(ws, offer_code: str, campaign_planner: str, lead: str, current_date: str) -> None:
    """Write the big header row (row 1)."""
    header_txt = f"[[{offer_code}] [CP: {campaign_planner}] [LEAD: {lead}] [DATE: {current_date}]"
    ws["A1"] = header_txt
    ws["A1"].font = Font(size=18)


def _write_consolidated_table(ws, conditions: pd.DataFrame, compiled_groups: List[Tuple[str, List[Tuple[str, pd.DataFrame]]]], starting_pops: Dict[str, int]) -> None:
    """Render the classic side-by-side consolidated sheet (rows begin at 2)."""

    # Column titles – row 2
    for cidx, title in enumerate(_BASE_TITLES, start=1):
        cell = ws.cell(row=2, column=cidx, value=title)
        cell.font = Font(size=12)
        cell.alignment = Alignment(wrap_text=True, horizontal="center", vertical="center")

    start_col = len(_BASE_TITLES) + 1  # first metric column

    # Metric header rows (rows 2-3)
    col_ptr = start_col
    for group_name, _ in compiled_groups:
        ws.merge_cells(start_row=2, start_column=col_ptr, end_row=2, end_column=col_ptr + len(_METRIC_ORDER) - 1)
        gcell = ws.cell(row=2, column=col_ptr, value=group_name)
        gcell.font = Font(size=10, bold=True)
        gcell.alignment = Alignment(horizontal="center", vertical="center")

        for m_idx, (_, disp) in enumerate(_METRIC_ORDER):
            cell = ws.cell(row=3, column=col_ptr + m_idx, value=disp)
            cell.font = Font(size=9)
            cell.fill = _HEADER_FILL
            cell.alignment = Alignment(horizontal="center", vertical="center")
            ws.column_dimensions[cell.column_letter].width = 14

        col_ptr += len(_METRIC_ORDER) + 1  # +1 spacer

    # Build lookup: group -> check_name -> metrics dict
    group_lookup: Dict[str, Dict[str, Dict[str, Any]]] = {}
    for grp, compiled in compiled_groups:
        sub: Dict[str, Dict[str, Any]] = {}
        for _, df in compiled:
            for _, row in df.iterrows():
                sub[row["check_name"]] = row.to_dict()
        group_lookup[grp] = sub

    # Starting population row (row 4)
    row_idx = 4
    ws.cell(row=row_idx, column=5, value="Starting Population").font = Font(size=10, bold=True)

    col_ptr = len(_BASE_TITLES) + _metric_remaining_offset()  # Remaining column within block
    for grp, _ in compiled_groups:
        val = starting_pops.get(grp, "")
        ws.cell(row=row_idx, column=col_ptr, value=val).font = Font(size=10)
        col_ptr += len(_METRIC_ORDER) + 1

    # Conditions rows – begin at row 5
    for _, cond_row in conditions.reset_index().iterrows():
        row_idx += 1
        # Base descriptive columns
        ws.cell(row=row_idx, column=1, value=cond_row["Section"]).font = Font(size=10)
        ws.cell(row=row_idx, column=2, value=cond_row["Template"]).font = Font(size=10)
        ws.cell(row=row_idx, column=3, value=cond_row["#"]).font = Font(size=10)
        ws.cell(row=row_idx, column=4, value=cond_row["sql"]).font = Font(size=10)
        ws.cell(row=row_idx, column=5, value=cond_row["description"]).font = Font(size=10)

        col_ptr = len(_BASE_TITLES) + 1
        for grp, _ in compiled_groups:
            metrics = group_lookup.get(grp, {}).get(cond_row["check_name"], {})
            for metric_key, _ in _METRIC_ORDER:
                val = metrics.get(metric_key, "")
                ws.cell(row=row_idx, column=col_ptr, value=val).font = Font(size=10)
                col_ptr += 1
            # spacer
            col_ptr += 1


def _write_group_table(
    ws,
    *,
    start_row: int,
    group_name: str,
    compiled: List[Tuple[str, pd.DataFrame]],
    conditions: pd.DataFrame,
    start_pop: int | None,
) -> int:
    """Render *compiled* metrics for **one** group starting at *start_row*.

    Returns the next free row index after the table.
    """

    # Title rows ---------------------------------------------------------
    # Row with base titles
    for cidx, title in enumerate(_BASE_TITLES, start=1):
        cell = ws.cell(row=start_row, column=cidx, value=title)
        cell.font = Font(size=12)
        cell.alignment = Alignment(wrap_text=True, horizontal="center", vertical="center")

    # Group title merge
    ws.merge_cells(start_row=start_row, start_column=len(_BASE_TITLES) + 1,
                   end_row=start_row, end_column=len(_BASE_TITLES) + len(_METRIC_ORDER))
    gcell = ws.cell(row=start_row, column=len(_BASE_TITLES) + 1, value=group_name)
    gcell.font = Font(size=10, bold=True)
    gcell.alignment = Alignment(horizontal="center", vertical="center")

    # Row with metric headers
    row_hdr2 = start_row + 1
    for m_idx, (_, disp) in enumerate(_METRIC_ORDER):
        cell = ws.cell(row=row_hdr2, column=len(_BASE_TITLES) + 1 + m_idx, value=disp)
        cell.font = Font(size=9)
        cell.fill = _HEADER_FILL
        cell.alignment = Alignment(horizontal="center", vertical="center")
        ws.column_dimensions[cell.column_letter].width = 14

    # Build lookup for metrics
    metrics_lookup: Dict[str, Dict[str, Any]] = {}
    for _, df in compiled:
        for _, row in df.iterrows():
            metrics_lookup[row["check_name"]] = row.to_dict()

    # Starting population row
    row_ptr = row_hdr2 + 1
    ws.cell(row=row_ptr, column=5, value="Starting Population").font = Font(size=10, bold=True)
    if start_pop is not None:
        pop_col = len(_BASE_TITLES) + _metric_remaining_offset()
        ws.cell(row=row_ptr, column=pop_col, value=start_pop).font = Font(size=10)

    # Condition rows
    for _, cond_row in conditions.reset_index().iterrows():
        row_ptr += 1
        ws.cell(row=row_ptr, column=1, value=cond_row["Section"]).font = Font(size=10)
        ws.cell(row=row_ptr, column=2, value=cond_row["Template"]).font = Font(size=10)
        ws.cell(row=row_ptr, column=3, value=cond_row["#"]).font = Font(size=10)
        ws.cell(row=row_ptr, column=4, value=cond_row["sql"]).font = Font(size=10)
        ws.cell(row=row_ptr, column=5, value=cond_row["description"]).font = Font(size=10)

        for m_idx, (mkey, _) in enumerate(_METRIC_ORDER):
            val = metrics_lookup.get(cond_row["check_name"], {}).get(mkey, "")
            ws.cell(row=row_ptr, column=len(_BASE_TITLES) + 1 + m_idx, value=val).font = Font(size=10)

        # Apply row fill based on presence only after writing cells across row
        if has_hist and not has_cur:
            row_fill = _HIST_ONLY_FILL
        elif has_cur and not has_hist:
            row_fill = _CUR_ONLY_FILL
        else:
            row_fill = None

        if row_fill is not None:
            for col in range(1, col_pct + block):
                ws.cell(row=row_ptr, column=col).fill = row_fill

    return row_ptr + 2  # leave a blank row after table


def _metric_remaining_offset() -> int:
    """Return 1-based offset (within metric block) of 'remaining' column."""
    for idx, (key, _) in enumerate(_METRIC_ORDER, start=1):
        if key == "remaining":
            return idx
    return 1

# ──────────────────────────────────────────────────────────────────────────────
# New comparison writer --------------------------------------------------------
# ──────────────────────────────────────────────────────────────────────────────


def _build_metrics_lookup(compiled: List[Tuple[str, pd.DataFrame]]) -> Dict[str, Dict[str, Any]]:
    """Return mapping check_name -> metric dict for one compiled list."""
    lookup: Dict[str, Dict[str, Any]] = {}
    for _, df in compiled:
        for _, row in df.iterrows():
            lookup[row["check_name"]] = row.to_dict()
    return lookup


def _ordered_union(list_a: Iterable[str], list_b: Iterable[str]) -> List[str]:
    """Return *list_a* followed by any items in *list_b* not already seen, preserving order."""
    seen = set()
    out: List[str] = []
    for item in list_a:
        if item not in seen:
            seen.add(item)
            out.append(item)
    for item in list_b:
        if item not in seen:
            seen.add(item)
            out.append(item)
    return out


def _write_group_comparison_table(
    ws,
    *,
    start_row: int,
    group_name: str,
    compiled_current: List[Tuple[str, pd.DataFrame]],
    compiled_historic: List[Tuple[str, pd.DataFrame]],
    conditions: pd.DataFrame,
    start_pop_current: int | None,
    start_pop_historic: int | None,
    historic_date: str = "",
) -> None:
    """Render side-by-side comparison table for one group."""

    # Column layout constants
    base_cols = len(_BASE_TITLES)
    block = len(_METRIC_ORDER)
    spacer = 1

    # Column indices (1-based)
    col_hist = base_cols + 1
    col_cur = col_hist + block + spacer
    col_diff = col_cur + block + spacer
    col_pct = col_diff + block + spacer

    # Header rows -------------------------------------------------------
    # Row with base titles
    for cidx, title in enumerate(_BASE_TITLES, start=1):
        cell = ws.cell(row=start_row, column=cidx, value=title)
        cell.font = Font(size=12)
        cell.alignment = Alignment(wrap_text=True, horizontal="center", vertical="center")

    # Top merged headers for each block
    def _mk_block(col_start, title):
        ws.merge_cells(start_row=start_row, start_column=col_start,
                       end_row=start_row, end_column=col_start + block - 1)
        cell = ws.cell(row=start_row, column=col_start, value=title)
        cell.font = Font(size=10, bold=True, color="FFFFFF")
        cell.alignment = Alignment(horizontal="center", vertical="center")
        cell.fill = _BLOCK_HEADER_FILL

    _mk_block(col_hist, f"Historical {historic_date or ''}")
    _mk_block(col_cur, "Current")
    _mk_block(col_diff, "Δ (Curr-Hist)")
    _mk_block(col_pct, "% Change")

    # Row with metric names
    hdr2 = start_row + 1
    for base_col in [col_hist, col_cur, col_diff, col_pct]:
        for m_idx, (_, disp) in enumerate(_METRIC_ORDER):
            cell = ws.cell(row=hdr2, column=base_col + m_idx, value=disp)
            cell.font = Font(size=9)
            cell.fill = _HEADER_FILL
            cell.alignment = Alignment(horizontal="center", vertical="center")
            ws.column_dimensions[cell.column_letter].width = 14


    # Build look-ups and ordered lists
    lookup_cur = _build_metrics_lookup(compiled_current)
    lookup_hist = _build_metrics_lookup(compiled_historic)

    hist_list = list(lookup_hist.keys())
    cur_list = list(lookup_cur.keys())

    # ------------------------------------------------------------------
    # Interleaved row sequence according to spec:
    #   – walk both lists; if names equal → combined row
    #   – else output hist-only row followed by cur-only row
    # ------------------------------------------------------------------
    merged_sequence: list[tuple[str, bool, bool]] = []  # (check_name, has_hist, has_cur)

    i = j = 0
    while i < len(hist_list) or j < len(cur_list):
        h_name = hist_list[i] if i < len(hist_list) else None
        c_name = cur_list[j] if j < len(cur_list) else None

        if h_name is not None and c_name is not None and h_name == c_name:
            merged_sequence.append((h_name, True, True))
            i += 1
            j += 1
        else:
            if h_name is not None:
                merged_sequence.append((h_name, True, False))
                i += 1
            if c_name is not None:
                merged_sequence.append((c_name, False, True))
                j += 1

    # Starting population row ------------------------------------------
    row_ptr = hdr2 + 1
    ws.cell(row=row_ptr, column=5, value="Starting Population").font = Font(size=10, bold=True)

    # Helper to write a value block
    def _write_block(row, col_start, values):
        for m_idx, key in enumerate(_METRIC_ORDER):
            ws.cell(row=row, column=col_start + m_idx, value=values.get(key[0], "")).font = Font(size=10)

    # Starting pop values -> only Remaining metric column is relevant.
    hist_vals = {"remaining": start_pop_historic} if start_pop_historic is not None else {}
    cur_vals = {"remaining": start_pop_current} if start_pop_current is not None else {}

    _write_block(row_ptr, col_hist, hist_vals)
    _write_block(row_ptr, col_cur, cur_vals)

    # diff and pct for start pop
    if start_pop_historic is not None and start_pop_current is not None:
        diff_val = start_pop_current - start_pop_historic
        pct_val = diff_val / start_pop_historic if start_pop_historic else None
        _write_block(row_ptr, col_diff, {"remaining": diff_val})
        _write_block(row_ptr, col_pct, {"remaining": pct_val})

    # Condition rows ----------------------------------------------------
    cond_lookup = conditions.reset_index().set_index('check_name')

    for chk_name, has_hist, has_cur in merged_sequence:
        row_ptr += 1
        # Descriptive columns – fall back to blanks if unknown in config
        if chk_name in cond_lookup.index:
            row_data = cond_lookup.loc[chk_name]
            ws.cell(row=row_ptr, column=1, value=row_data['Section']).font = Font(size=10)
            ws.cell(row=row_ptr, column=2, value=row_data['Template']).font = Font(size=10)
            ws.cell(row=row_ptr, column=3, value=row_data['#']).font = Font(size=10)
            ws.cell(row=row_ptr, column=4, value=row_data['sql']).font = Font(size=10)
            ws.cell(row=row_ptr, column=5, value=row_data['description']).font = Font(size=10)
        else:
            ws.cell(row=row_ptr, column=3, value=chk_name).font = Font(size=10)  # at least display name

        hist_metrics = lookup_hist.get(chk_name, {}) if has_hist else {}
        cur_metrics = lookup_cur.get(chk_name, {}) if has_cur else {}

        _write_block(row_ptr, col_hist, hist_metrics)
        _write_block(row_ptr, col_cur, cur_metrics)

        # Compute diff / pct per metric
        diff_vals = {}
        pct_vals = {}
        for m_key, _ in _METRIC_ORDER:
            h_val = hist_metrics.get(m_key)
            c_val = cur_metrics.get(m_key)
            if h_val is not None and c_val is not None:
                try:
                    diff = c_val - h_val
                    diff_vals[m_key] = diff
                    pct_vals[m_key] = diff / h_val if h_val else ''
                except Exception:
                    pass
        _write_block(row_ptr, col_diff, diff_vals)
        _write_block(row_ptr, col_pct, pct_vals)

    # Borders around full data rectangle
    from openpyxl.styles import Border, Side
    thin = Side(style="thin", color="000000")
    border = Border(left=thin, right=thin, top=thin, bottom=thin)

    last_col = col_pct + block - 1
    for col in range(1, last_col + 1):
        # top
        ws.cell(row=start_row, column=col).border = border
        # bottom
        ws.cell(row=row_ptr, column=col).border = border
    for row in range(start_row, row_ptr + 1):
        ws.cell(row=row, column=1).border = border
        ws.cell(row=row, column=last_col).border = border

    # Freeze panes below the two header rows (i.e., first data row)
    ws.freeze_panes = ws.cell(row=hdr2 + 1, column=1)

    # leave blank row after table
    return
engines/__init__.py::47
"""
Core processing engines for tlptaco v2.
"""old_waterfall/waterfall.py::20993
from typing import Dict, List, Any, Optional
from tlptaco.construct_sql.construct_sql import SQLConstructor
from tlptaco.connections.teradata import TeradataHandler
from tlptaco.logging.logging import call_logger, CustomLogger
from tlptaco.eligibility.eligibility import Eligible
from tlptaco.validations.waterfall import WaterfallValidator
from tlptaco.validations.general import BaseValidator
import pandas as pd
from collections import OrderedDict
from openpyxl import Workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from openpyxl.styles import Font, PatternFill, Alignment
from datetime import datetime


class Waterfall(BaseValidator, WaterfallValidator):
    """
    A class to generate and analyze waterfall reports for campaign eligibility checks.

    Attributes:
        conditions (pd.DataFrame): Conditions for eligibility checks.
        offer_code (str): The offer code.
        campaign_planner (str): The campaign planner.
        lead (str): The lead person.
        waterfall_location (str): The location to save the waterfall report.
        _sqlconstructor (SQLConstructor): An instance of SQLConstructor to build SQL queries.
        _teradata_connection (Optional[TeradataHandler]): The Teradata connection handler.
        _column_names (OrderedDict): Column names for the waterfall report.
        _query_results (Dict[str, List[pd.DataFrame]]): Query results for each identifier.
        _compiled_dataframes (OrderedDict): Compiled dataframes for each identifier.
        _starting_population (Optional[int]): The starting population for the waterfall analysis.
        _combined_df (Optional[pd.DataFrame]): Combined dataframe for the waterfall report.
    """
    _validators = {
        'conditions': WaterfallValidator.validate_conditions,
        'offer_code': WaterfallValidator.validate_offer_code,
        'campaign_planner': WaterfallValidator.validate_campaign_planner,
        'lead': WaterfallValidator.validate_lead,
        'waterfall_location': WaterfallValidator.validate_waterfall_location,
        '_sqlconstructor': WaterfallValidator.validate_sqlconstructor,
        'logger': BaseValidator.validate_logger,
        '_teradata_connection': BaseValidator.validate_teradata_connection
    }

    def __init__(
            self,
            conditions: Dict[str, Dict[str, Any]],
            offer_code: str,
            campaign_planner: str,
            lead: str,
            waterfall_location: str,  # TODO: add to check to see if this directory exists
            sql_constructor: SQLConstructor,
            logger: CustomLogger,
            teradata_connection: Optional[TeradataHandler] = None
    ) -> None:
        """
        Initializes the waterfall class with the provided parameters.

        Args:
            conditions (Dict[str, Dict[str, Any]]): Conditions for eligibility checks.
            offer_code (str): The offer code.
            campaign_planner (str): The campaign planner.
            lead (str): The lead person.
            waterfall_location (str): The location to save the waterfall report.
            sql_constructor (SQLConstructor): An instance of SQLConstructor to build SQL queries.
            teradata_connection (Optional[TeradataHandler]): The Teradata connection handler.
        """
        self.logger = logger
        self.current_date = datetime.now().strftime("%Y-%m-%d %H_%M_%S")
        self._sqlconstructor = sql_constructor
        self._teradata_connection = teradata_connection
        self.offer_code = offer_code
        self.campaign_planner = campaign_planner
        self.lead = lead
        self.waterfall_location = waterfall_location
        self.conditions = conditions

        # prep column names
        self._column_names = OrderedDict({
            'unique_drops': '{identifier} drop if only this drop',
            'increm_drops': '{identifier} drop increm',
            'cumul_drops': '{identifier} drop cumul',
            'regain': '{identifier} regain if no scrub',
            'remaining': '{identifier} remaining'
        })

        # initializing properties
        self._query_results = dict()
        self._compiled_dataframes = OrderedDict()
        self._starting_population = None
        self._combined_df = None

    @classmethod
    def from_eligible(cls, eligibility: Eligible, waterfall_location: str) -> "Waterfall":
        conditions = cls._prepare_conditions(eligibility.conditions)
        offer_code = eligibility.offer_code
        campaign_planner = eligibility.campaign_planner
        lead = eligibility.lead
        sql_constructor = eligibility._sqlconstructor
        logger = eligibility.logger
        teradata_connection = eligibility._teradata_connection

        return cls(conditions, offer_code, campaign_planner, lead, waterfall_location, sql_constructor, logger,
                   teradata_connection)

    @property
    def campaign_planner(self):
        return self._campaign_planner

    @campaign_planner.setter
    def campaign_planner(self, value):
        self._campaign_planner = value

    @property
    def offer_code(self):
        return self._offer_code

    @offer_code.setter
    def offer_code(self, value):
        self._offer_code = value

    @property
    def lead(self):
        return self._lead

    @lead.setter
    def lead(self, value):
        self._lead = value

    @property
    def waterfall_location(self):
        return self._waterfall_location

    @waterfall_location.setter
    def waterfall_location(self, value):
        self._waterfall_location = value

    @classmethod
    def _prepare_conditions(cls, conditions: dict) -> dict:
        """
        Prepares the conditions by transforming them into a dictionary.

        Args:
            conditions (Dict[str, Dict[str, Any]]): Conditions for eligibility checks.

        Returns:
            dict: The prepared conditions as a dictionary.
        """
        result_dict = {}
        for channel, templates in conditions.items():
            for template, checks in templates.items():
                for check in checks:
                    column_name = check.get('column_name', None)
                    description = check.get('description', None)
                    sql = check.get('sql', None)
                    if column_name is not None:
                        modified_description = f'[{template}] {description}' if description else None
                        result_dict[column_name] = {
                            'sql': sql,
                            'description': modified_description
                        }
        return result_dict

    @property
    def conditions(self) -> dict:
        """Getter for conditions."""
        return self._conditions

    @conditions.setter
    def conditions(self, value: Dict[str, Any]) -> None:
        """Setter for conditions."""
        self._conditions = value

    def _save_results(self, identifier: str, data: pd.DataFrame) -> None:
        """
        Saves the results of a query to the _query_results dictionary.

        Args:
            identifier (str): The identifier for the query results.
            data (pd.DataFrame): The data to save.
        """
        # make sure the columns are in order (orderby check number)
        data = data[list(sorted(data.columns, key=lambda x: int(x.split('_')[-1])))]
        if self._query_results.get(identifier) is None:
            self._query_results[identifier] = []
        self._query_results[identifier].append(data)

    @call_logger()
    def _calculate_regain(self) -> None:
        """Calculates the regain SQL and saves the results to the _query_results dictionary."""
        queries = self._sqlconstructor.waterfall.generate_regain_sql()
        # save queries
        self.logger.info(f'{self.__class__}._calculate_regain {queries=}')
        for identifier, query in queries.items():
            if self._teradata_connection is not None:
                df = self._teradata_connection.to_pandas(query)
                df['Index'] = self._column_names.get('regain').format(identifier=identifier)
                df = df.set_index('Index')
                self.logger.info(f"{self.__class__}._calculate_regain {identifier=} {df.to_dict()}")
                self._save_results(identifier, df)

    @call_logger()
    def _calculate_incremental_drops(self) -> None:
        """Calculates the incremental drops SQL and saves the results to the _query_results dictionary."""
        queries = self._sqlconstructor.waterfall.generate_incremental_drops_sql()
        # save queries
        self.logger.info(f'{self.__class__}._calculate_incremental_drops {queries=}')
        for identifier, query in queries.items():
            if self._teradata_connection is not None:
                df = self._teradata_connection.to_pandas(query)
                df['Index'] = self._column_names.get('increm_drops').format(identifier=identifier)
                df = df.set_index('Index')
                self.logger.info(f"{self.__class__}._calculate_incremental_drops {identifier=} {df.to_dict()}")
                self._save_results(identifier, df)

    @call_logger()
    def _calculate_unique_drops(self) -> None:
        """Calculates the unique drops SQL and saves the results to the _query_results dictionary."""
        queries = self._sqlconstructor.waterfall.generate_unique_drops_sql()
        # save queries
        self.logger.info(f'{self.__class__}._calculate_unique_drops {queries=}')
        for identifier, query in queries.items():
            if self._teradata_connection is not None:
                df = self._teradata_connection.to_pandas(query)
                df['Index'] = self._column_names.get('unique_drops').format(identifier=identifier)
                df = df.set_index('Index')
                self.logger.info(f"{self.__class__}._calculate_unique_drops {identifier=} {df.to_dict()}")
                self._save_results(identifier, df)

    @call_logger()
    def _calculate_remaining(self) -> None:
        """Calculates the remaining SQL and saves the results to the _query_results dictionary."""
        queries = self._sqlconstructor.waterfall.generate_remaining_sql()
        # save queries
        self.logger.info(f'{self.__class__}._calculate_remaining {queries=}')
        for identifier, query in queries.items():
            if self._teradata_connection is not None:
                df = self._teradata_connection.to_pandas(query)
                df['Index'] = self._column_names.get('remaining').format(identifier=identifier)
                df = df.set_index('Index')
                self.logger.info(f"{self.__class__}._calculate_remaining {identifier=} {df.to_dict()}")
                self._save_results(identifier, df)

    def _step1_create_base_tables(self):
        """Creates a summary table for each set of unique identifiers provided, based on the eligibility"""
        queries = self._sqlconstructor.waterfall.generate_unique_identifier_details_sql()
        self.logger.info(f'{self.__class__}._step1_create_base_tables {queries=}')
        for identifier, details in queries.items():
            query = details.get('sql')
            table_name = details.get('table_name')
            collect_query = details.get('collect_stats')
            self.logger.debug(f'--{self.__class__}._step1_create_base_tables\n--SQL for {identifier} stats table\n'
                              f'--{table_name}\n%s;\n--Collect Query\n{collect_query};', query)
            if self._teradata_connection is not None:
                self._teradata_connection.execute_query(query)
                self.logger.info(f"{self.__class__}._step1_create_base_tables created backend table {table_name}")
                self._teradata_connection.tracking.track_table(table_name)
                self.logger.info(f"{self.__class__}._step1_create_base_tables collect statistics on {table_name}")
                self._teradata_connection.execute_query(collect_query)

    def _step2_calculate_stats(self):
        """Grabs the stats from the base tables created in step1 above"""
        queries = self._sqlconstructor.waterfall.generate_all_sql()
        for identifier, query in queries.items():
            self.logger.debug(f'--{self.__class__}._step2_calculate_stats\n--SQL for {identifier} Waterfall Counts\n%s',
                              query)

            unique_drop = self._column_names.get('unique_drops').format(identifier=identifier)
            increm_drop = self._column_names.get('increm_drops').format(identifier=identifier)
            cumul_drop = self._column_names.get('cumul_drops').format(identifier=identifier)
            regain = self._column_names.get('regain').format(identifier=identifier)
            remaining = self._column_names.get('remaining').format(identifier=identifier)

            column_names = {
                self._sqlconstructor.waterfall.generate_regain_sql.__name__: regain,
                self._sqlconstructor.waterfall.generate_incremental_drops_sql.__name__: increm_drop,
                self._sqlconstructor.waterfall.generate_remaining_sql.__name__: remaining,
                self._sqlconstructor.waterfall.generate_unique_drops_sql.__name__: unique_drop
            }

            if self._teradata_connection is not None:
                df = self._teradata_connection.to_pandas(query)
                df = df.T
                df.columns = df.iloc[0]
                df = df[1:]
                # rename the columns
                df = df.rename(columns=column_names, axis=1)
                # make sure the index is in the proper order
                ordered_index = list(sorted(df.index, key=lambda x: int(x.split('_')[-1])))
                df = df.reindex(ordered_index)

                # create cumulative drop column
                self._starting_population = df[increm_drop].values[0] + df[remaining].values[0]
                df[cumul_drop] = self._starting_population - df[remaining]

                df = df[[unique_drop, increm_drop, cumul_drop, regain, remaining]]

                starting_pop_df = pd.DataFrame({
                    unique_drop: 0,
                    increm_drop: 0,
                    cumul_drop: 0,
                    regain: 0,
                    remaining: self._starting_population},
                    index=['main_BA_0']
                )
                df = pd.concat([starting_pop_df, df])
                self.logger.info(f"{self.__class__}._step2_calculate_stats {identifier=}")
                self._compiled_dataframes[identifier] = df

    def _step3_create_excel(self):
        """Creates an excel sheet with the stats gathered in _step2 above"""
        # Create a workbook and add a worksheet
        wb = Workbook()
        ws = wb.active
        ws.title = 'waterfall'

        # Add header information
        header = f'[[{self.offer_code}] [CP: {self.campaign_planner}] [LEAD: {self.lead}] [DATE: {self.current_date}]'
        ws['A1'] = header
        ws['A1'].font = Font(size=18)

        # Add headers and Starting Population to A2:C2 and C3
        cell = ws.cell(row=2, column=1, value='Checks')
        cell.font = Font(size=12, name='Albany AMT')
        cell.alignment = Alignment(wrap_text=True, horizontal='center', vertical='center')

        cell = ws.cell(row=2, column=2, value='Criteria')
        cell.font = Font(size=12, name='Albany AMT')
        cell.alignment = Alignment(wrap_text=True, horizontal='center', vertical='center')

        cell = ws.cell(row=2, column=3, value='Description')
        cell.font = Font(size=12, name='Albany AMT')
        cell.alignment = Alignment(wrap_text=True, horizontal='center', vertical='center')

        cell = ws.cell(row=3, column=3, value='Starting Population')
        cell.font = Font(size=12, name='Albany AMT')
        cell.alignment = Alignment(wrap_text=True, horizontal='center', vertical='center')

        # turn conditions into DataFrame and make sure the column order is correct
        conditions_df = pd.DataFrame.from_dict(self.conditions, orient='index')
        conditions_df = conditions_df[['sql', 'description']]

        # put all df's into one list to prep for merge
        all_dfs = [conditions_df] + list(self._compiled_dataframes.values())

        df_combined = None
        for df in all_dfs:
            if df_combined is None:
                df_combined = df
            else:
                df_combined = pd.merge(df_combined, df, left_index=True, right_index=True)
        df_combined.reset_index(drop=False, inplace=True)

        conditions_df = conditions_df.reset_index(drop=False)

        # write the first dataframe values starting from cell A4
        for r_idx, row in enumerate(dataframe_to_rows(conditions_df, index=False, header=False), start=4):
            for c_idx, value in enumerate(row, start=1):
                ws.cell(row=r_idx, column=c_idx, value=value)

        # write other dataframes starting from appropriate columns and add headers
        start_col = 5
        for key, df in self._compiled_dataframes.items():
            for col_num, value in enumerate(df.columns, start=start_col):
                cell = ws.cell(row=2, column=col_num)
                cell.value = value
                cell.fill = PatternFill(start_color='87CEEB', end_color='87CEEB', fill_type='solid')
                cell.font = Font(size=9, name='Albany AMT')
                cell.alignment = Alignment(wrap_text=True, horizontal='center', vertical='center')
                ws.column_dimensions[cell.column_letter].width = 11
            for r_idx, row in enumerate(dataframe_to_rows(df, index=False, header=False), start=3):
                for c_idx, value in enumerate(row, start=start_col):
                    cell = ws.cell(row=r_idx, column=c_idx, value=value)
                    cell.font = Font(size=10, name='Albany AMT')
                    cell.number_format = '#,##'
            start_col += len(df.columns) + 1

        channel_name_prev = ''
        header_fill = PatternFill(start_color='ADD8E6', end_color='ADD8E6', fill_type='solid')
        col_a_fill = PatternFill(start_color='BBBFAC', end_color='BBBFAC', fill_type='solid')
        blank_fill = PatternFill(start_color='364C40', end_color='364C40', fill_type='solid')

        # loop through the A column, extract the check numbers, and insert a blank row between new channels
        row_modifier = 0  # used to keep count consistent when new rows are inserted
        for row in range(4, len(df_combined) + 5):
            row += row_modifier
            cell = ws.cell(row=row, column=1)
            if cell.value is None:
                continue
            parts = cell.value.split('_')
            channel_name = parts[0]
            check_number = parts[-1]
            if channel_name != channel_name_prev and row != 4:
                ws.insert_rows(row)
                ws.cell(row=row, column=2, value=channel_name.upper())
                # set the background color for this new row
                for col in range(1, start_col):
                    ws.cell(row=row, column=col).fill = header_fill
                row_modifier += 1
            cell.value = check_number
            channel_name_prev = channel_name

        # Apply header fill color to row 2
        for col in range(1, start_col):
            ws.cell(row=2, column=col).fill = header_fill

        # Apply color to column A starting from row 3
        for row in range(3, len(df_combined) + 5):
            ws.cell(row=row, column=1).fill = col_a_fill

        # Apply color to blank rows and columns
        # for row in range(4, len(df_combined) + 5):
        #     if ws.cell(row=row, column=3).value is None:
        #         for col in range(1, start_col):
        #             ws.cell(row=row, column=col).fill = header_fill

        for col in range(1, start_col):
            if ws.cell(row=4, column=col).value is None:
                for row in range(1, len(df_combined) + 5):
                    ws.cell(row=row, column=col).fill = blank_fill

        # set column widths
        ws.column_dimensions['A'].width = 9
        ws.column_dimensions['B'].width = 41
        ws.column_dimensions['C'].width = 51

        # Save the Excel file
        wb.save(f'{self.waterfall_location}/{self.offer_code}_waterfall_{self.current_date}.xlsx')

    def generate_waterfall(self):
        """
        Runs all 3 steps at once
        """
        self._step1_create_base_tables()
        self._step2_calculate_stats()
        self._step3_create_excel()old_waterfall/construct_sql.py::4780
import re
from typing import Union, List, Set, Dict, Any
from tlptaco.validations.exceptions import ValueWarning


class ConstructSQLMeta(type):
    """
    Metaclass for validating the structure of the '_unique_identifiers' attribute and ensuring it adheres to specific
    formatting rules. Also includes methods for extracting table aliases and performing validations.
    """

    def __setattr__(cls, name: str, value: Any) -> None:
        """
        Overrides the __setattr__ method to include validation checks for specific attributes.

        Args:
            name (str): The name of the attribute.
            value (Any): The value of the attribute.

        Raises:
            ValueError: If the attribute value is invalid.
        """
        if name == '_unique_identifiers':
            if value is not None:
                cls.validate_unique_identifiers(cls, value)
        super().__setattr__(cls, name, value)

    @staticmethod
    def _extract_table_aliases(tables: Dict[str, List[Dict[str, Any]]]) -> Set[str]:
        """
        Extracts table aliases from the provided tables dictionary.

        Args:
            tables (Dict[str, List[Dict[str, Any]]]): The tables from which to extract aliases.

        Returns:
            Set[str]: A set of table aliases.
        """
        table_aliases: Set[str] = set()
        for table in tables.get('tables', []):
            table_alias = table.get('alias')
            if table_alias not in table_aliases:
                table_aliases.add(table_alias)
        return table_aliases

    def validate_unique_identifiers(cls, value: Union[List[str], Set[str]]) -> None:
        """
        Validates the structure and format of the '_unique_identifiers' attribute.

        Args:
            value (Union[List[str], Set[str]]): The unique identifiers to validate.

        Raises:
            ValueError: If the structure or format is invalid.
        """
        if not isinstance(value, (list, set)):
            raise ValueError("_unique_identifiers must be a list or set of strings.")

        unique_identifiers: Set[str] = set(value) if isinstance(value, list) else value
        pattern = re.compile(r'^[a-zA-Z]+\.[a-zA-Z0-9_]+$')
        columns_seen: Dict[str, str] = {}
        table_aliases = cls._extract_table_aliases(cls._tables)

        for identifier in unique_identifiers:
            parts = [part.strip() for part in identifier.split(',')]
            for part in parts:
                if not pattern.match(part):
                    raise ValueError(f"Invalid identifier format: {part}")

                alias, column = part.split('.')
                if alias not in table_aliases:
                    raise ValueError(f"Alias '{alias}' not present in _table_aliases.")

                if column in columns_seen and columns_seen[column] != alias:
                    raise ValueError(f"Column '{column}' is used with multiple aliases.")
                columns_seen[column] = alias


class EligibilityConstructSQLValidator:
    _validators = {}

    def __setattr__(self, name, value):
        try:
            if name in self._validators:
                self._validators[name](value)
                if hasattr(self, 'logger') and self.logger:
                    self.logger.info(f'{self.__class__}.{name} validated')
            super().__setattr__(name, value)
        except ValueWarning as e:
            if hasattr(self, 'logger') and self.logger:
                self.logger.warning(f'WARNING {self.__class__}.{name}: {e}')
            super().__setattr__(name, value)
        except Exception as e:
            if hasattr(self, 'logger') and self.logger:
                self.logger.error(f'{self.__class__}.{name} unable to validate: {e}')

    @staticmethod
    def validate_work_tables(tables):
        # make sure the sql queries do not contain an semi-colons
        for table in tables:
            sql: str = table.get('sql')
            if ';' in sql:
                sql_list = sql.split(';')
                # remove empty items
                sql_list = [x for x in sql_list if x.replace(' ', '') not in ('', '\n', '\t')]
                # if there is only one query present, then remove the semicolon
                if len(sql_list) == 1:
                    sql = sql.replace(';', '')
                    table['sql'] = sql
                    raise ValueWarning("User work tables cannot contain semi-colons; attempting to remove")
                else:
                    raise ValueError(
                        f"User work tables cannot contain multiple queries or semicolons; please ensure each user work table only contains 1 query")old_waterfall/construct_sql/waterfall.py::15708
import inspect
from typing import Dict, List, Any, Optional
from tlptaco.logging.logging import call_logger, CustomLogger


class WaterfallSQLConstructor:
    """
    A class to generate SQL queries for the waterfall process in eligibility checks.
    This class is handled by the `tlptaco.construct_sql.construct_sql.SQLConstructor` class and is used by `tlptaco.waterfall.waterfall.Waterfall`.
    Generates Teradata SQL based on requirements provided by `tlptaco.waterfall.waterfall.Waterfall`.

    Attributes:
        conditions (Dict[str, Dict[str, Any]]): Conditions for eligibility checks.
        _backend_tables (Dict[str, str]): Backend table details.
        parsed_unique_identifiers (Dict[str, Any]): Parsed unique identifiers.
        _conditions_column_mappings (Dict[str, Any]): Mappings of conditions to columns.
        _regain_sql (Optional[Dict[str, str]]): SQL queries for regaining records.
        _incremental_drops_sql (Optional[Dict[str, str]]): SQL queries for incremental drops.
        _unique_drops_sql (Optional[Dict[str, str]]): SQL queries for unique drops.
        _remaining_sql (Optional[Dict[str, str]]): SQL queries for remaining records.
        logger (CustomLogger): logger used to log
    """

    def __init__(
            self,
            conditions: Dict[str, Dict[str, Any]],
            conditions_column_mappings: Dict[str, Any],
            backend_tables: Dict[str, str],
            parsed_unique_identifiers: Dict[str, Any],
            logger: CustomLogger
    ) -> None:
        """
        Initializes class

        :param conditions: contains conditions for the waterfall
        :type conditions: Dict
        :param conditions_column_mappings: contains the related checks for each check found in conditions
        :type conditions_column_mappings: Dict
        :param backend_tables: the tables that will contain the waterfall counts for each unique identifier (i.e. party id, account number, etc.)
        :type backend_tables: Dict
        :param parsed_unique_identifiers: various formats (e.g. with table alias, without table alias) of the unique identifiers that the waterfall will use for counts
        :type parsed_unique_identifiers: Dict
        :param logger: logger to use for logging
        :type logger: CustomLogger
        """
        self.logger = logger
        self.conditions = conditions
        self._backend_tables = backend_tables
        self.parsed_unique_identifiers = parsed_unique_identifiers
        self._conditions_column_mappings = conditions_column_mappings
        self._column_names = self._extract_column_names(conditions)
        self._regain_sql = None
        self._incremental_drops_sql = None
        self._unique_drops_sql = None
        self._remaining_sql = None

    @call_logger()
    def generate_unique_identifier_details_sql(self) -> Dict[str, Dict[str, str]]:
        """
        Generates the SQL for waterfall counts for each unique identifier provided

        :return: SQL queries to create the tables for each unique identifier waterfall counts
        :rtype: Dict[str, Dict[str, str]]
        """
        queries: Dict[str, Dict[str, str]] = {}
        column_names: List[str] = []
        for channel, templates in self.conditions.items():
            for template, checks in templates.items():
                for check in checks:
                    column_names.append(check.get('column_name'))

        max_columns = [f'\nMAX({check}) AS max_{check}' for check in column_names]
        select_sql = max_columns.copy()

        for identifier in self.parsed_unique_identifiers.get('original_without_aliases', []):
            group_by = [str(x) for x in range(1, len(identifier.split('.')) + 1)]
            identifier_details_table = self._backend_tables.get(identifier)
            sql = f"""
CREATE TABLE {identifier_details_table} AS (
    SELECT
        {identifier},
        {','.join(select_sql)}
    FROM
        {self._backend_tables.get('eligibility')}
    GROUP BY {','.join(group_by)}
) WITH DATA PRIMARY INDEX ({identifier})
"""
            collect_stats = f'COLLECT STATISTICS INDEX prindx ON {identifier_details_table}'
            queries[identifier] = {
                'sql': sql,
                'table_name': identifier_details_table,
                'collect_stats': collect_stats
            }
        return queries

    @staticmethod
    def _extract_column_names(conditions: dict) -> List:
        column_names = []
        for channel, templates in conditions.items():
            for template, checks in templates.items():
                for check in checks:
                    column_names.append(check.get('column_name'))

        # sort the columns by their column number (i.e. the last number in the string)
        column_names = sorted(column_names, key=lambda x: int(x.split('_')[-1]))
        return column_names

    @call_logger()
    def generate_unique_drops_sql(self) -> Dict[str, str]:
        """
        Generate the SQL to generate the counts for Unique Drops

        :return: SQL queries to identify counts for each unique identifier
        :rtype: Dict[str, str]
        """
        queries: Dict[str, str] = {}
        for identifier in self.parsed_unique_identifiers.get('original_without_aliases', []):
            case_statements: List[str] = []
            conditions_keys = self._conditions_column_mappings.keys()
            # sort the conditions by the last value found in the column name (i.e. the check number)
            conditions_keys = sorted(conditions_keys, key=lambda x: int(x.split('_')[-1]))
            for check in conditions_keys:
                case_statement = f"SUM(CASE WHEN max_{check} = 0 THEN 1 ELSE 0 END) AS {check}"
                case_statements.append(case_statement)

            query = f"SELECT\n CAST('{inspect.currentframe().f_code.co_name}' AS VARCHAR(30)) AS stat_name,\n"
            query += ',\n'.join(case_statements)
            query += f'\nFROM {self._backend_tables.get(identifier)}'

            queries[identifier] = query

        self._unique_drops_sql = queries
        return queries

    @call_logger()
    def generate_regain_sql(self) -> Dict[str, str]:
        """
        Generate the SQL to generate the counts for number regained if a condition is removed

        :return: SQL queries to identify counts for each unique identifier
        :rtype: Dict[str, str]
        """
        # This method's implementation was not present in the provided screenshots.
        queries: Dict[str, str] = {}
        return queries

    @call_logger()
    def generate_incremental_drops_sql(self) -> Dict[str, str]:
        """
        SQL for generating the counts for the number of entities incrementally dropped for each check

        :return: SQL queries to identify counts for each unique identifier
        :rtype: Dict[str, str]
        """
        queries: Dict[str, str] = {}
        for identifier in self.parsed_unique_identifiers.get('original_without_aliases', []):
            case_statements: List[str] = []

            # MAIN WATERFALL CASE STATEMENTS
            main_checks = self.conditions.get('main').get('BA')
            main_checks = [x.get('column_name') for x in main_checks]
            main_checks_list = list()
            for col in main_checks:
                temp_list = [f'max_{col} = 0']
                temp_list.extend(main_checks_list)
                statement = f"SUM(CASE WHEN {' AND '.join(temp_list)} THEN 1 END) AS {col}"
                case_statements.append(statement)
                main_checks_list.append(f'max_{col} = 1')

            # prep main_checks_list for use in channels
            main_checks_list = [f'max_{col} = 1' for col in main_checks]

            # CHANNEL STATEMENTS
            channels = [x for x in self.conditions.keys() if x != 'main']
            for channel in channels:
                channel_dict = self.conditions.get(channel)
                channel_templates = channel_dict.keys()

                if 'BA' in channel_templates:
                    channel_base_list = list()
                    channel_base_checks = [check.get('column_name') for check in channel_dict.get('BA')]
                    for col in channel_base_checks:
                        temp_list = [f'max_{col} = 0']
                        temp_list.extend(channel_base_list)
                        temp_list.extend(main_checks_list)
                        statement = f"SUM(CASE WHEN {' AND '.join(temp_list)} THEN 1 END) AS {col}"
                        case_statements.append(statement)
                        channel_base_list.append(f'max_{col} = 1')
                    # prep channel_base_list for use in templates
                    channel_base_list = [f'max_{col} = 1' for col in channel_base_checks]
                else:
                    channel_base_list = main_checks_list.copy()

                previous_templates_list = list()
                for template in [x for x in channel_templates if x != 'BA']:
                    channel_segment_checks = [check.get('column_name') for check in channel_dict.get(template)]
                    for col in channel_segment_checks:
                        temp_list = [f'max_{x} = 1' if x != col else f'max_{x} = 0' for x in channel_segment_checks]
                        temp_list.extend(channel_base_list)

                        if previous_templates_list:
                            temp_prevs = list()
                            for prev in previous_templates_list:
                                temp_prev = f"({' OR '.join(prev)})"
                                temp_prevs.append(temp_prev)
                            temp_statement = f" AND {' AND '.join(temp_prevs)}"
                            statement = f"SUM(CASE WHEN {' AND '.join(temp_list)}{temp_statement} THEN 1 END) AS {col}"
                        else:
                            statement = f"SUM(CASE WHEN {' AND '.join(temp_list)} THEN 1 END) AS {col}"
                        case_statements.append(statement)

                    # prep list for previous_templates_list
                    temp_list = [f'max_{x} = 0' for x in channel_segment_checks]
                    previous_templates_list.append(temp_list.copy())

            # CREATE QUERY
            query = f"SELECT\n CAST('{inspect.currentframe().f_code.co_name}' AS VARCHAR(30)) AS stat_name,\n"
            query += ',\n'.join(case_statements)
            query += f'\nFROM {self._backend_tables.get(identifier)}'

            queries[identifier] = query

        self._incremental_drops_sql = queries
        return queries

    @call_logger()
    def generate_remaining_sql(self) -> Dict[str, str]:
        """
        Generate the SQL to count the entities remaining after each check

        :return: SQL queries to identify counts for each unique identifier
        :rtype: Dict[str, str]
        """
        queries: Dict[str, str] = {}
        for identifier in self.parsed_unique_identifiers.get('original_without_aliases', []):
            case_statements: List[str] = []

            # MAIN WATERFALL CASE STATEMENTS
            main_checks = self.conditions.get('main').get('BA')
            main_checks = [x.get('column_name') for x in main_checks]
            main_checks_list = list()
            for col in main_checks:
                temp_list = [f'max_{col} = 1']
                temp_list.extend(main_checks_list)
                statement = f"SUM(CASE WHEN {' AND '.join(temp_list)} THEN 1 END) AS {col}"
                case_statements.append(statement)
                main_checks_list.append(f'max_{col} = 1')

            # CHANNEL STATEMENTS
            channels = [x for x in self.conditions.keys() if x != 'main']
            for channel in channels:
                channel_dict = self.conditions.get(channel)
                channel_templates = channel_dict.keys()
                if 'BA' in channel_templates:
                    channel_base_list = list()
                    channel_base_checks = [check.get('column_name') for check in channel_dict.get('BA')]
                    for col in channel_base_checks:
                        temp_list = [f'max_{col} = 1']
                        temp_list.extend(channel_base_list)
                        temp_list.extend(main_checks_list)
                        statement = f"SUM(CASE WHEN {' AND '.join(temp_list)} THEN 1 END) AS {col}"
                        case_statements.append(statement)
                        channel_base_list.append(f'max_{col} = 1')
                else:
                    channel_base_list = main_checks_list.copy()

                previous_templates_list = list()
                for template in [x for x in channel_templates if x != 'BA']:
                    channel_segment_list = list()
                    channel_segment_checks = [check.get('column_name') for check in channel_dict.get(template)]
                    for col in channel_segment_checks:
                        temp_list = [f'max_{col} = 1']
                        temp_list.extend(channel_segment_list)
                        temp_list.extend(channel_base_list)
                        temp_list.extend(main_checks_list)

                        if previous_templates_list:
                            temp_prevs = list()
                            for prev in previous_templates_list:
                                temp_statement = f"({' OR '.join(prev)})"
                                temp_prevs.append(temp_statement)
                            temp_statement = f" AND {' AND '.join(temp_prevs)}"
                            statement = f"SUM(CASE WHEN {' AND '.join(temp_list)}{temp_statement} THEN 1 END) AS {col}"
                        else:
                            statement = f"SUM(CASE WHEN {' AND '.join(temp_list)} THEN 1 END) AS {col}"
                        case_statements.append(statement)
                    channel_segment_list.append(f'max_{col} = 1')

                    # prep template for following templates
                    previous_templates_list.append([f'max_{col} = 0' for col in channel_segment_checks])

            query = f"SELECT\n CAST('{inspect.currentframe().f_code.co_name}' AS VARCHAR(30)) AS stat_name,\n"
            query += ',\n'.join(case_statements)
            query += f'\nFROM {self._backend_tables.get(identifier)}'
            queries[identifier] = query

        self._remaining_sql = queries
        return queries

    def generate_all_sql(self) -> Dict:
        """
        Generates all the SQL for all the waterfall counts

        :return: SQL queries to identify ALL counts for each unique identifier
        :rtype: Dict
        """
        remain_sql = self.generate_remaining_sql()
        increm_sql = self.generate_incremental_drops_sql()
        unique_sql = self.generate_unique_drops_sql()
        regain_sql = self.generate_regain_sql()
        queries = dict()
        for identifier in self.parsed_unique_identifiers.get('original_without_aliases', []):
            regain = regain_sql.get(identifier)
            increm = increm_sql.get(identifier)
            remain = remain_sql.get(identifier)
            unique = unique_sql.get(identifier)

            query = f"""
{regain}
UNION ALL
{increm}
UNION ALL
{remain}
UNION ALL
{unique}
"""
            queries[identifier] = query

        return queriesiostream/loader.py::341
"""
Input file loader utilities.
"""
import pandas as pd

def read_csv(path: str, **kwargs) -> pd.DataFrame:
    return pd.read_csv(path, **kwargs)

def read_excel(path: str, **kwargs) -> pd.DataFrame:
    return pd.read_excel(path, **kwargs)

def read_parquet(path: str, **kwargs) -> pd.DataFrame:
    return pd.read_parquet(path, **kwargs)iostream/writer.py::556
"""
Output file writer utilities.
"""
import os
from pathlib import Path

def write_dataframe(df, path: str, fmt: str, **kwargs):
    p = Path(path)
    p.parent.mkdir(parents=True, exist_ok=True)
    if fmt == "csv":
        df.to_csv(path, index=False, **kwargs)
    elif fmt in ("excel", "xlsx"):
        df.to_excel(path, index=False, **kwargs)
    elif fmt == "parquet":
        df.to_parquet(path, index=False, **kwargs)
    # create .end file
    end_path = str(p.with_suffix(".end"))
    with open(end_path, "w") as f:
        f.write(str(len(df)))iostream/__init__.py::37
"""
I/O utilities for tlptaco v2.
"""