"""
Pydantic models for tlptaco configuration with built-in validation.
"""
from pydantic import BaseModel, model_validator, field_validator, ConfigDict, Field
from typing import List, Dict, Optional, Any, Union
import re

# --- Base Models ---

class ConditionCheck(BaseModel):
    # name is autogenerated if not provided
    name: Optional[str] = None
    sql: str
    description: Optional[str] = None

class TemplateConditions(BaseModel):
    """
    Conditions for a given channel or main: a BA (base) filter and one or more segments.
    Users may define segments directly alongside BA; legacy 'others' key is also supported.
    """
    # allow legacy 'others' key and arbitrary segment keys
    model_config = ConfigDict(extra="allow")
    BA: List[ConditionCheck]
    # segments: mapping segment_name -> list of checks
    segments: Dict[str, List[ConditionCheck]] = {}

    @model_validator(mode='before')
    def parse_segments(cls, data):  # type: ignore[name-defined]
        # Extract user-defined segments: legacy 'others' dict or keys beside 'BA'
        if not isinstance(data, dict):
            return data
        raw = data.copy()
        segs: Dict[str, Any] = {}
        # legacy 'others'
        if 'others' in raw:
            old = raw.pop('others') or {}
            if isinstance(old, dict):
                segs.update(old)
        # new-style: any key != 'BA'
        for key in list(raw.keys()):
            if key == 'BA':
                continue
            # treat as segment
            segs[key] = raw.pop(key)
        raw['segments'] = segs
        return raw
    
    @property
    def others(self) -> Dict[str, List[ConditionCheck]]:
        """Legacy alias for segments mapping."""
        return self.segments

class ConditionsConfig(BaseModel):
    """
    All conditions configuration: main BA filters and per-channel BA filters + segments.
    Automatically assigns unique names to each check and enforces structure.
    """
    main: TemplateConditions
    channels: Dict[str, TemplateConditions]

    @model_validator(mode='after')
    def assign_names_and_validate(cls, self) -> 'ConditionsConfig':  # type: ignore[name-defined]
        # Assign autogenerated names for main BA checks
        # main channel key is 'main'
        # BA filters for main
        for idx, chk in enumerate(self.main.BA, start=1):
            chk.name = f"main_BA_{idx}"
        # main should not have any segments
        if self.main.segments:
            raise ValueError("'main' conditions may not include segments; only BA checks are allowed")
        # Process each channel: assign names for BA filters and optional segments
        for channel, tmpl in self.channels.items():
            # BA filters for channel
            for idx, chk in enumerate(tmpl.BA, start=1):
                chk.name = f"{channel}_BA_{idx}"
            # segments for channel (optional)
            for seg_name, checks in tmpl.segments.items():
                for idx, chk in enumerate(checks, start=1):
                    chk.name = f"{channel}_{seg_name}_{idx}"
        return self

class TableConfig(BaseModel):
    name: str
    alias: str
    sql: Optional[str] = None  # Made optional as it might not always be used
    join_type: Optional[str]
    join_conditions: Optional[str]
    where_conditions: Optional[str]
    unique_index: Optional[str]
    collect_stats: Optional[List[str]]
    # Ensure alias is a valid identifier (starts with letter/_ and contains only alphanumeric/_)
    @field_validator('alias', mode='before')
    def validate_alias(cls, v):  # type: ignore[name-defined]
        if not isinstance(v, str) or not re.match(r'^[A-Za-z_][A-Za-z0-9_]*$', v):
            raise ValueError(
                f"Invalid alias '{v}'. Must start with a letter or underscore and contain only alphanumeric characters or underscores."
            )
        return v

class OutputOptions(BaseModel):
    format: str
    additional_arguments: Optional[Dict[str, Any]] = {}
    custom_function: Optional[str]

# --- Config Sections with Validation ---

class EligibilityConfig(BaseModel):
    eligibility_table: Optional[str] = None
    conditions: ConditionsConfig
    tables: List[TableConfig]
    unique_identifiers: List[str]

    @model_validator(mode='after')
    def check_identifier_aliases_are_valid(self) -> 'EligibilityConfig':
        valid_aliases = {t.alias for t in self.tables}
        for identifier in self.unique_identifiers:
            if '.' in identifier:
                alias = identifier.split('.')[0]
                if alias not in valid_aliases:
                    raise ValueError(
                        f"In 'eligibility.unique_identifiers', '{identifier}' uses an invalid alias '{alias}'. "
                        f"Valid aliases are: {valid_aliases}"
                    )
        return self
    # Validate eligibility_table naming: identifier or schema.table
    @field_validator('eligibility_table', mode='before')
    def validate_eligibility_table(cls, v):  # type: ignore[name-defined]
        if v is None:
            return v
        if not isinstance(v, str) or not re.match(
            r'^[A-Za-z_][A-Za-z0-9_]*(\.[A-Za-z_][A-Za-z0-9_]*)?$', v
        ):
            raise ValueError(
                f"Invalid eligibility_table '{v}'. Must be a valid identifier or schema.table."
            )
        return v

# -----------------------------------------------------------------------------
# Waterfall configuration & history tracking
# -----------------------------------------------------------------------------


class WaterfallHistoryConfig(BaseModel):
    """Optional SQLite history tracking for each waterfall run.

    Two *alternative* ways to choose the comparison run:

    1. ``recent_window_days`` (alias ``lookback_days`` – legacy):
       pick the **latest** run whose timestamp is within the last *N* days.
    2. ``compare_offset_days`` (alias ``days_ago_to_compare``):
       pick the run whose timestamp is **closest to exactly N days ago**.
       When this value is supplied it takes precedence over
       ``recent_window_days``.
    """

    model_config = ConfigDict(populate_by_name=True)

    # Toggle history tracking on / off (default off for backwards-compat)
    track: bool = False
    # Optional explicit path to the SQLite DB (directory will be created).
    # When *None*, a file named ``waterfall_history.sqlite`` is created
    # inside ``waterfall.output_directory``.
    db_path: Optional[str] = None

    # Window-style selection (legacy name: lookback_days)
    recent_window_days: Optional[int] = Field(30, alias="lookback_days")

    # Point-in-time offset selection (legacy name: days_ago_to_compare)
    compare_offset_days: Optional[int] = Field(None, alias="days_ago_to_compare")

    @model_validator(mode="after")
    def _validate_numbers(cls, self):  # type: ignore[name-defined]
        if self.track and self.recent_window_days is None and self.compare_offset_days is None:
            # Default window when nothing specified
            self.recent_window_days = 30

        if self.recent_window_days is not None and self.recent_window_days <= 0:
            raise ValueError("recent_window_days must be a positive integer")

        if self.compare_offset_days is not None and self.compare_offset_days <= 0:
            raise ValueError("compare_offset_days must be a positive integer if supplied")
        return self


class WaterfallConfig(BaseModel):
    output_directory: str
    count_columns: List[Union[str, List[str]]]
    # New nested history configuration with sane defaults
    history: WaterfallHistoryConfig = WaterfallHistoryConfig()

class OutputChannelConfig(BaseModel):
    columns: List[str]
    file_location: str
    file_base_name: str
    output_options: OutputOptions
    unique_on: Optional[List[str]] = []

    @model_validator(mode='after')
    def check_unique_on_are_in_columns(cls, self) -> 'OutputChannelConfig':  # type: ignore[name-defined]
        """
        Ensure that simple unique_on column names exist in the output columns.
        If the unique_on element contains a dot, assume it's a qualified identifier and skip validation.
        """
        if self.unique_on:
            # If all unique_on entries are unqualified (no dot), enforce subset
            simple_keys = [u for u in self.unique_on if '.' not in u]
            if simple_keys:
                missing = set(simple_keys) - set(self.columns)
                if missing:
                    raise ValueError(
                        f"'unique_on' columns {missing} are not present in the selected 'columns'."
                    )
        return self

# -----------------------------------------------------------------------------
# Failed Records dump configuration
# -----------------------------------------------------------------------------


class FailedRecordsConfig(BaseModel):
    """Configuration for optional failed-records export."""

    enabled: bool = False
    # When true, only *one* failure reason per unique identifier is kept –
    # the first encountered by reason rank.
    first_reason_only: bool = False
    file_location: str
    file_base_name: str
    output_options: OutputOptions


class OutputConfig(BaseModel):
    channels: Dict[str, OutputChannelConfig]
    # Optional failed-records dump configuration
    failed_records: Optional[FailedRecordsConfig] = None

class LoggingConfig(BaseModel):
    level: str
    file: Optional[str]
    debug_file: Optional[str]
    # New dedicated SQL log file capturing rendered SQL from templates
    sql_file: Optional[str] = None
    # Optional list of section prefixes to *exclude* from the SQL log.
    # e.g. ["waterfall", "output"] will suppress those sections.
    sql_exclude_sections: Optional[List[str]] = []

class DatabaseConfig(BaseModel):
    host: str = "rchtera"
    user: str
    password: Optional[str]
    logmech: Optional[str] = "KRB5"

# -----------------------------------------------------------------------------
# Pre-SQL configuration --------------------------------------------------------
# -----------------------------------------------------------------------------


class PreSQLAnalytics(BaseModel):
    """Optional analytics executed after a pre-SQL file.

    Example YAML::

        pre_sql:
          - path: prepare_tables.sql
            analytics:
              table: sandbox.tmp_eligibility
              unique_counts:
                - customer_id
                - [company_id, site_id]
    """

    table: str
    # Each item either a single column name or a list of column names
    unique_counts: List[Union[str, List[str]]]


class PreSQLFile(BaseModel):
    """Single pre-SQL file configuration (path + optional analytics)."""

    path: str
    analytics: Optional[PreSQLAnalytics] = None

    @model_validator(mode="before")
    def _coerce_legacy(cls, v):  # type: ignore[name-defined]
        """Allow legacy string-only usage (path as bare string)."""
        if isinstance(v, str):
            return {"path": v}
        if isinstance(v, dict):
            return v
        raise TypeError("Invalid pre_sql item: must be str or mapping")

# --- Top-Level App Config with Cross-Section Validation ---

class AppConfig(BaseModel):
    """
    Top-level application configuration, including offer code and campaign metadata.
    """
    # Offer code displayed in CLI progress bar (defaults to 'Running')
    offer_code: str = "Running"
    # Campaign metadata for waterfall report header
    campaign_planner: str = ""
    lead: str = ""
    logging: LoggingConfig
    database: DatabaseConfig
    eligibility: EligibilityConfig
    waterfall: WaterfallConfig
    output: OutputConfig
    # Optional list of pre-sql file definitions (legacy: list of strings)
    pre_sql: Optional[List[PreSQLFile]] = None

    # ------------------------------------------------------------------
    # Automatic defaults population (before main validation)
    # ------------------------------------------------------------------

    @model_validator(mode='before')
    def _populate_defaults(cls, data):  # type: ignore[name-defined]
        if not isinstance(data, dict):
            return data

        offer = data.get('offer_code') or 'run'

        # ---- eligibility_table default ---------------------------------
        elig = data.get('eligibility')
        if isinstance(elig, dict) and not elig.get('eligibility_table'):
            elig['eligibility_table'] = f"user_wpb.{offer}_elig"

        # ---- ensure database.host default is applied when key missing --
        db_cfg = data.get('database')
        if isinstance(db_cfg, dict) and 'host' not in db_cfg:
            db_cfg['host'] = 'rchtera'

        return data

    @field_validator('pre_sql', mode='after')
    def _validate_pre_sql_paths(cls, v):  # type: ignore[name-defined]
        if not v:
            return v
        import os
        missing = [p.path for p in v if not os.path.isfile(p.path)]
        if missing:
            raise ValueError(f"pre_sql files not found: {missing}")
        return v

    @model_validator(mode='after')
    def check_cross_config_dependencies(self) -> 'AppConfig':
        # 1. Check WaterfallConfig -> EligibilityConfig dependency
        valid_ids = {uid.split('.')[-1] for uid in self.eligibility.unique_identifiers}
        waterfall_ids = set()
        for item in self.waterfall.count_columns:
            cols = [item] if isinstance(item, str) else item
            for col in cols:
                waterfall_ids.add(col.split('.')[-1])
        if not waterfall_ids.issubset(valid_ids):
            invalid_cols = waterfall_ids - valid_ids
            raise ValueError(
                f"Waterfall 'count_columns' contain invalid identifiers: {invalid_cols}. "
                f"They must be a subset of eligibility 'unique_identifiers': {valid_ids}"
            )

        # 2. Check OutputConfig -> EligibilityConfig dependency (for aliases)
        valid_aliases = {t.alias for t in self.eligibility.tables}
        for channel, out_cfg in self.output.channels.items():
            for column in out_cfg.columns:
                if '.' in column:
                    alias = column.split('.')[0]
                    if alias not in valid_aliases:
                        raise ValueError(
                            f"In output channel '{channel}', column '{column}' uses an invalid alias '{alias}'. "
                            f"Valid aliases are: {valid_aliases}"
                        )
        return self
